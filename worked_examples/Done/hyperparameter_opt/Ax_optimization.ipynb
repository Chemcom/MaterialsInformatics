{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8178ad1",
   "metadata": {},
   "source": [
    "# Bayesian Optimization with Ax Tutorial\n",
    "\n",
    "Ax can be used for any optimization task but is particularily suited for parameter tuning. This tutorial demonstrates how to use Bayesian Optimization with the Ax library to optimize machine learning model parameters and other experiments efficiently. Ax is a powerful tool developed by Meta for automating experimentation and optimization.\n",
    "\n",
    "## What is Bayesian Optimization?\n",
    "\n",
    "Bayesian Optimization is an efficient strategy for optimizing black-box functions that are expensive to evaluate. It's particularly well-suited for high-dimensional spaces and situations where sampling data is costly.\n",
    "\n",
    "#### Video (general Bayesian Optimization)\n",
    "\n",
    "https://www.youtube.com/watch?v=qVEBO1Viv7k&list=PLL0SWcFqypCl4lrzk1dMWwTUrzQZFt7y0&index=39 (Bayesian Optimization)\n",
    "\n",
    "## Introduction to Ax\n",
    "\n",
    "Ax is an accessible, modular, and efficient library that supports both gradient-based optimization and Bayesian Optimization. It is designed to automate the process of optimizing complex experiments, like tuning hyperparameters for machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aa56809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ax-platform in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: botorch==0.11.0 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ax-platform) (0.11.0)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ax-platform) (3.1.4)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ax-platform) (2.2.2)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ax-platform) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ax-platform) (1.5.1)\n",
      "Requirement already satisfied: ipywidgets in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ax-platform) (8.1.3)\n",
      "Requirement already satisfied: plotly>=5.12.0 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ax-platform) (5.22.0)\n",
      "Requirement already satisfied: typeguard in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ax-platform) (2.13.3)\n",
      "Requirement already satisfied: pyre-extensions in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ax-platform) (0.0.30)\n",
      "Requirement already satisfied: multipledispatch in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from botorch==0.11.0->ax-platform) (1.0.0)\n",
      "Requirement already satisfied: mpmath<=1.3,>=0.19 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from botorch==0.11.0->ax-platform) (1.3.0)\n",
      "Requirement already satisfied: torch>=1.13.1 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from botorch==0.11.0->ax-platform) (2.3.1)\n",
      "Requirement already satisfied: pyro-ppl>=1.8.4 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from botorch==0.11.0->ax-platform) (1.9.1)\n",
      "Requirement already satisfied: gpytorch==1.11 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from botorch==0.11.0->ax-platform) (1.11)\n",
      "Requirement already satisfied: linear-operator==0.5.1 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from botorch==0.11.0->ax-platform) (0.5.1)\n",
      "Requirement already satisfied: jaxtyping>=0.2.9 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from linear-operator==0.5.1->botorch==0.11.0->ax-platform) (0.2.33)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from plotly>=5.12.0->ax-platform) (8.5.0)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from plotly>=5.12.0->ax-platform) (24.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ipywidgets->ax-platform) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ipywidgets->ax-platform) (8.26.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ipywidgets->ax-platform) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ipywidgets->ax-platform) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ipywidgets->ax-platform) (3.0.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from jinja2->ax-platform) (2.1.5)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from pandas->ax-platform) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from pandas->ax-platform) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from pandas->ax-platform) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from pandas->ax-platform) (2024.1)\n",
      "Requirement already satisfied: typing-inspect in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from pyre-extensions->ax-platform) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from pyre-extensions->ax-platform) (4.12.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from scikit-learn->ax-platform) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from scikit-learn->ax-platform) (3.5.0)\n",
      "Requirement already satisfied: decorator in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->ax-platform) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->ax-platform) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->ax-platform) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->ax-platform) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->ax-platform) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->ax-platform) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->ax-platform) (4.9.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from pyro-ppl>=1.8.4->botorch==0.11.0->ax-platform) (3.3.0)\n",
      "Requirement already satisfied: pyro-api>=0.1.1 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from pyro-ppl>=1.8.4->botorch==0.11.0->ax-platform) (0.1.2)\n",
      "Requirement already satisfied: tqdm>=4.36 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from pyro-ppl>=1.8.4->botorch==0.11.0->ax-platform) (4.66.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->ax-platform) (1.16.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from torch>=1.13.1->botorch==0.11.0->ax-platform) (3.15.4)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from torch>=1.13.1->botorch==0.11.0->ax-platform) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from torch>=1.13.1->botorch==0.11.0->ax-platform) (3.3)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from torch>=1.13.1->botorch==0.11.0->ax-platform) (2024.6.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from typing-inspect->pyre-extensions->ax-platform) (1.0.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->ax-platform) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->ax-platform) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->ax-platform) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->ax-platform) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->ax-platform) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/miniconda3/envs/masterEnv2/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->ax-platform) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "# Installation\n",
    "!pip install ax-platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d257042e",
   "metadata": {},
   "source": [
    "# Basic Ax Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407d5b7",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "After installing Ax, we need to import the necessary libraries to set up our optimization problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "749ea947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Ax libraries\n",
    "from ax.service.ax_client import AxClient\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc5ac43",
   "metadata": {},
   "source": [
    "## Defining the Optimization Problem\n",
    "This notebook will show you two application of optimization. First we will optimize a single function and then we will optimize the hyperparameters of a Random Forest Model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3520065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective_function(parameters):\n",
    "    x = parameters.get(\"x\")\n",
    "    y = parameters.get(\"y\")\n",
    "    objective_value = (x - 0.5)**2 + (y - 0.5)**2  # Simple convex function\n",
    "    return {\"objective_value\": (objective_value, 0.0)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7088a4bd",
   "metadata": {},
   "source": [
    "## Configuring Bayesian Optimization in Ax\n",
    "\n",
    "We will now configure the Bayesian Optimization process in Ax by setting up an experiment. The evaluation function calculates an objective value based on the parameters x and y. We are using a simple quadratic function for this problem. This function matches our originally defined objective function. The optimize function takes a few inputs. The first is a list of parameters to be optimized. Each is given a name, type, and optimization bounds. Next, you have to define an evaluation function for the model to use to evaluate the generated parameters. Next is the objective to be optimized. This is the name of the value. This needs to match the name defined in the objective function. Next, tell the model if it's trying to minimize or maximize. Lastly you can specify the number of trials it needs to run. The higher this number is the more accurate it will be. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d87a90ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 08-02 11:11:02] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 08-02 11:11:02] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter y. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 08-02 11:11:02] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='x', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='y', parameter_type=FLOAT, range=[0.0, 1.0])], parameter_constraints=[]).\n",
      "[INFO 08-02 11:11:02] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.\n",
      "[INFO 08-02 11:11:02] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=2 num_trials=None use_batch_trials=False\n",
      "[INFO 08-02 11:11:02] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=5\n",
      "[INFO 08-02 11:11:02] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=5\n",
      "[INFO 08-02 11:11:02] ax.modelbridge.dispatch_utils: `verbose`, `disable_progbar`, and `jit_compile` are not yet supported when using `choose_generation_strategy` with ModularBoTorchModel, dropping these arguments.\n",
      "[INFO 08-02 11:11:02] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials]). Iterations after 5 will take longer to generate due to model-fitting.\n",
      "[INFO 08-02 11:11:02] ax.service.managed_loop: Started full optimization with 15 steps.\n",
      "[INFO 08-02 11:11:02] ax.service.managed_loop: Running optimization trial 1...\n",
      "[INFO 08-02 11:11:03] ax.service.managed_loop: Running optimization trial 2...\n",
      "[INFO 08-02 11:11:03] ax.service.managed_loop: Running optimization trial 3...\n",
      "[INFO 08-02 11:11:03] ax.service.managed_loop: Running optimization trial 4...\n",
      "[INFO 08-02 11:11:03] ax.service.managed_loop: Running optimization trial 5...\n",
      "[INFO 08-02 11:11:03] ax.service.managed_loop: Running optimization trial 6...\n",
      "[INFO 08-02 11:11:03] ax.service.managed_loop: Running optimization trial 7...\n",
      "[INFO 08-02 11:11:03] ax.service.managed_loop: Running optimization trial 8...\n",
      "[INFO 08-02 11:11:04] ax.service.managed_loop: Running optimization trial 9...\n",
      "[INFO 08-02 11:11:04] ax.service.managed_loop: Running optimization trial 10...\n",
      "[INFO 08-02 11:11:04] ax.service.managed_loop: Running optimization trial 11...\n",
      "[INFO 08-02 11:11:05] ax.service.managed_loop: Running optimization trial 12...\n",
      "[INFO 08-02 11:11:05] ax.service.managed_loop: Running optimization trial 13...\n",
      "[INFO 08-02 11:11:06] ax.service.managed_loop: Running optimization trial 14...\n",
      "[INFO 08-02 11:11:06] ax.service.managed_loop: Running optimization trial 15...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'x': 0.4952375380767063, 'y': 0.5022554343381569}\n"
     ]
    }
   ],
   "source": [
    "# Correct way to initialize AxClient for Bayesian Optimization with objective setup\n",
    "from ax.service.managed_loop import optimize\n",
    "\n",
    "def evaluation_function(parameterization):\n",
    "    x = parameterization.get(\"x\")\n",
    "    y = parameterization.get(\"y\")\n",
    "    return (x - 0.5)**2 + (y - 0.5)**2  # Example objective function\n",
    "\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=[\n",
    "        {\"name\": \"x\", \"type\": \"range\", \"bounds\": [0.0, 1.0]},\n",
    "        {\"name\": \"y\", \"type\": \"range\", \"bounds\": [0.0, 1.0]},\n",
    "    ],\n",
    "    evaluation_function=evaluation_function,\n",
    "    objective_name='objective_value',\n",
    "    minimize=True,  # Specifies that we are minimizing our objective\n",
    "    total_trials=15\n",
    ")\n",
    "\n",
    "print(\"Best Parameters:\", best_parameters)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d31528",
   "metadata": {},
   "source": [
    "## Using Ax to Optimize Within a Dataset\n",
    "\n",
    "First we need to import our libraries. The biggest new one that we need is the euclidean_distances. This is useful when trying to optimize from values within a dataset. It gives the model an 'evaluation function' to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45419177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from ax.service.managed_loop import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f466b3d8",
   "metadata": {},
   "source": [
    "Next we need to import and clean up our data. It's important to do this step as it can cause the Ax model to run into errors down the line or be unable to efficiently find the optimal value. Let's inspect our data and see what we're working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bce34d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QAgNO3(%)</th>\n",
       "      <th>Qpva(%)</th>\n",
       "      <th>Qtsc(%)</th>\n",
       "      <th>Qseed(%)</th>\n",
       "      <th>Qtot(uL/min)</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3295.000000</td>\n",
       "      <td>3295.000000</td>\n",
       "      <td>3295.000000</td>\n",
       "      <td>3295.000000</td>\n",
       "      <td>3295.000000</td>\n",
       "      <td>3295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.904572</td>\n",
       "      <td>24.408302</td>\n",
       "      <td>8.781410</td>\n",
       "      <td>6.810531</td>\n",
       "      <td>706.117347</td>\n",
       "      <td>0.502575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.492450</td>\n",
       "      <td>8.571230</td>\n",
       "      <td>7.492459</td>\n",
       "      <td>4.503125</td>\n",
       "      <td>207.889927</td>\n",
       "      <td>0.198356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.530000</td>\n",
       "      <td>9.999518</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.498852</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>0.131345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18.529201</td>\n",
       "      <td>17.880991</td>\n",
       "      <td>3.999102</td>\n",
       "      <td>4.000384</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>0.321620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.099136</td>\n",
       "      <td>22.819075</td>\n",
       "      <td>5.848561</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>807.580000</td>\n",
       "      <td>0.491461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.399194</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.660701</td>\n",
       "      <td>815.000000</td>\n",
       "      <td>0.661212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>42.809816</td>\n",
       "      <td>40.001015</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>983.000000</td>\n",
       "      <td>0.910637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         QAgNO3(%)      Qpva(%)      Qtsc(%)     Qseed(%)  Qtot(uL/min)  \\\n",
       "count  3295.000000  3295.000000  3295.000000  3295.000000   3295.000000   \n",
       "mean     23.904572    24.408302     8.781410     6.810531    706.117347   \n",
       "std       8.492450     8.571230     7.492459     4.503125    207.889927   \n",
       "min       4.530000     9.999518     0.500000     0.498852    200.000000   \n",
       "25%      18.529201    17.880991     3.999102     4.000384    600.000000   \n",
       "50%      25.099136    22.819075     5.848561     6.500000    807.580000   \n",
       "75%      30.500000    32.399194    12.000000     8.660701    815.000000   \n",
       "max      42.809816    40.001015    30.500000    19.500000    983.000000   \n",
       "\n",
       "              loss  \n",
       "count  3295.000000  \n",
       "mean      0.502575  \n",
       "std       0.198356  \n",
       "min       0.131345  \n",
       "25%       0.321620  \n",
       "50%       0.491461  \n",
       "75%       0.661212  \n",
       "max       0.910637  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('AgNP_dataset.csv')\n",
    "\n",
    "# Drop or fill NaN values\n",
    "data = data.dropna()  # Option 1: Drop rows with NaN values\n",
    "# or\n",
    "data = data.fillna(data.mean())  # Option 2: Fill NaN values with column mean\n",
    "\n",
    "# Define the inputs (X) and the output (y)\n",
    "y = data['loss']\n",
    "X = data.drop('loss', axis=1)\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b128e",
   "metadata": {},
   "source": [
    "3295 is a great amount of data to be working with. We can see that our loss varies bewteen .131 and .910\n",
    "\n",
    "Next we need to create our model and test it. The optimize function uses very similar inputs as the model above, except this time it has a lot more parameters to optimize than before. Since we are optimizing within a dataset we can set the bounds to be the max and min of each input column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fa7db17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 08-02 11:11:07] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter QAgNO3(%). If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 08-02 11:11:07] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter Qpva(%). If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 08-02 11:11:07] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter Qtsc(%). If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 08-02 11:11:07] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter Qseed(%). If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 08-02 11:11:07] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter Qtot(uL/min). If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 08-02 11:11:07] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='QAgNO3(%)', parameter_type=FLOAT, range=[4.53, 42.80981595]), RangeParameter(name='Qpva(%)', parameter_type=FLOAT, range=[9.999518096, 40.00101474]), RangeParameter(name='Qtsc(%)', parameter_type=FLOAT, range=[0.5, 30.5]), RangeParameter(name='Qseed(%)', parameter_type=FLOAT, range=[0.498851653, 19.5]), RangeParameter(name='Qtot(uL/min)', parameter_type=FLOAT, range=[200.0, 983.0])], parameter_constraints=[]).\n",
      "[INFO 08-02 11:11:07] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.\n",
      "[INFO 08-02 11:11:07] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=5 num_trials=None use_batch_trials=False\n",
      "[INFO 08-02 11:11:07] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=10\n",
      "[INFO 08-02 11:11:07] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=10\n",
      "[INFO 08-02 11:11:07] ax.modelbridge.dispatch_utils: `verbose`, `disable_progbar`, and `jit_compile` are not yet supported when using `choose_generation_strategy` with ModularBoTorchModel, dropping these arguments.\n",
      "[INFO 08-02 11:11:07] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 10 trials, BoTorch for subsequent trials]). Iterations after 10 will take longer to generate due to model-fitting.\n",
      "[INFO 08-02 11:11:07] ax.service.managed_loop: Started full optimization with 100 steps.\n",
      "[INFO 08-02 11:11:07] ax.service.managed_loop: Running optimization trial 1...\n",
      "[INFO 08-02 11:11:07] ax.service.managed_loop: Running optimization trial 2...\n",
      "[INFO 08-02 11:11:07] ax.service.managed_loop: Running optimization trial 3...\n",
      "[INFO 08-02 11:11:07] ax.service.managed_loop: Running optimization trial 4...\n",
      "[INFO 08-02 11:11:07] ax.service.managed_loop: Running optimization trial 5...\n",
      "[INFO 08-02 11:11:07] ax.service.managed_loop: Running optimization trial 6...\n",
      "[INFO 08-02 11:11:07] ax.service.managed_loop: Running optimization trial 7...\n",
      "[INFO 08-02 11:11:07] ax.service.managed_loop: Running optimization trial 8...\n",
      "[INFO 08-02 11:11:07] ax.service.managed_loop: Running optimization trial 9...\n",
      "[INFO 08-02 11:11:07] ax.service.managed_loop: Running optimization trial 10...\n",
      "[INFO 08-02 11:11:07] ax.service.managed_loop: Running optimization trial 11...\n",
      "[INFO 08-02 11:11:08] ax.service.managed_loop: Running optimization trial 12...\n",
      "[INFO 08-02 11:11:09] ax.service.managed_loop: Running optimization trial 13...\n",
      "[INFO 08-02 11:11:10] ax.service.managed_loop: Running optimization trial 14...\n",
      "[INFO 08-02 11:11:11] ax.service.managed_loop: Running optimization trial 15...\n",
      "[INFO 08-02 11:11:12] ax.service.managed_loop: Running optimization trial 16...\n",
      "[INFO 08-02 11:11:13] ax.service.managed_loop: Running optimization trial 17...\n",
      "[INFO 08-02 11:11:14] ax.service.managed_loop: Running optimization trial 18...\n",
      "[INFO 08-02 11:11:15] ax.service.managed_loop: Running optimization trial 19...\n",
      "[INFO 08-02 11:11:17] ax.service.managed_loop: Running optimization trial 20...\n",
      "[INFO 08-02 11:11:18] ax.service.managed_loop: Running optimization trial 21...\n",
      "[INFO 08-02 11:11:19] ax.service.managed_loop: Running optimization trial 22...\n",
      "[INFO 08-02 11:11:21] ax.service.managed_loop: Running optimization trial 23...\n",
      "[INFO 08-02 11:11:22] ax.service.managed_loop: Running optimization trial 24...\n",
      "[INFO 08-02 11:11:24] ax.service.managed_loop: Running optimization trial 25...\n",
      "[INFO 08-02 11:11:25] ax.service.managed_loop: Running optimization trial 26...\n",
      "[INFO 08-02 11:11:27] ax.service.managed_loop: Running optimization trial 27...\n",
      "[INFO 08-02 11:11:31] ax.service.managed_loop: Running optimization trial 28...\n",
      "[INFO 08-02 11:11:32] ax.service.managed_loop: Running optimization trial 29...\n",
      "[INFO 08-02 11:11:33] ax.service.managed_loop: Running optimization trial 30...\n",
      "[INFO 08-02 11:11:34] ax.service.managed_loop: Running optimization trial 31...\n",
      "[INFO 08-02 11:11:36] ax.service.managed_loop: Running optimization trial 32...\n",
      "[INFO 08-02 11:11:37] ax.service.managed_loop: Running optimization trial 33...\n",
      "[INFO 08-02 11:11:38] ax.service.managed_loop: Running optimization trial 34...\n",
      "[INFO 08-02 11:11:40] ax.service.managed_loop: Running optimization trial 35...\n",
      "[INFO 08-02 11:11:42] ax.service.managed_loop: Running optimization trial 36...\n",
      "[INFO 08-02 11:11:43] ax.service.managed_loop: Running optimization trial 37...\n",
      "[INFO 08-02 11:11:45] ax.service.managed_loop: Running optimization trial 38...\n",
      "[INFO 08-02 11:11:47] ax.service.managed_loop: Running optimization trial 39...\n",
      "[INFO 08-02 11:11:48] ax.service.managed_loop: Running optimization trial 40...\n",
      "[INFO 08-02 11:11:49] ax.service.managed_loop: Running optimization trial 41...\n",
      "[INFO 08-02 11:11:51] ax.service.managed_loop: Running optimization trial 42...\n",
      "[INFO 08-02 11:11:52] ax.service.managed_loop: Running optimization trial 43...\n",
      "[INFO 08-02 11:11:53] ax.service.managed_loop: Running optimization trial 44...\n",
      "[INFO 08-02 11:11:55] ax.service.managed_loop: Running optimization trial 45...\n",
      "[INFO 08-02 11:11:57] ax.service.managed_loop: Running optimization trial 46...\n",
      "[INFO 08-02 11:12:00] ax.service.managed_loop: Running optimization trial 47...\n",
      "[INFO 08-02 11:12:02] ax.service.managed_loop: Running optimization trial 48...\n",
      "[INFO 08-02 11:12:03] ax.service.managed_loop: Running optimization trial 49...\n",
      "[INFO 08-02 11:12:05] ax.service.managed_loop: Running optimization trial 50...\n",
      "[INFO 08-02 11:12:08] ax.service.managed_loop: Running optimization trial 51...\n",
      "[INFO 08-02 11:12:11] ax.service.managed_loop: Running optimization trial 52...\n",
      "[INFO 08-02 11:12:13] ax.service.managed_loop: Running optimization trial 53...\n",
      "[INFO 08-02 11:12:14] ax.service.managed_loop: Running optimization trial 54...\n",
      "[INFO 08-02 11:12:16] ax.service.managed_loop: Running optimization trial 55...\n",
      "[INFO 08-02 11:12:19] ax.service.managed_loop: Running optimization trial 56...\n",
      "[INFO 08-02 11:12:22] ax.service.managed_loop: Running optimization trial 57...\n",
      "[INFO 08-02 11:12:24] ax.service.managed_loop: Running optimization trial 58...\n",
      "[INFO 08-02 11:12:26] ax.service.managed_loop: Running optimization trial 59...\n",
      "[INFO 08-02 11:12:29] ax.service.managed_loop: Running optimization trial 60...\n",
      "[INFO 08-02 11:12:32] ax.service.managed_loop: Running optimization trial 61...\n",
      "[INFO 08-02 11:12:33] ax.service.managed_loop: Running optimization trial 62...\n",
      "[INFO 08-02 11:12:35] ax.service.managed_loop: Running optimization trial 63...\n",
      "[INFO 08-02 11:12:37] ax.service.managed_loop: Running optimization trial 64...\n",
      "[INFO 08-02 11:12:40] ax.service.managed_loop: Running optimization trial 65...\n",
      "[INFO 08-02 11:12:43] ax.service.managed_loop: Running optimization trial 66...\n",
      "[INFO 08-02 11:12:45] ax.service.managed_loop: Running optimization trial 67...\n",
      "[INFO 08-02 11:12:47] ax.service.managed_loop: Running optimization trial 68...\n",
      "[INFO 08-02 11:12:48] ax.service.managed_loop: Running optimization trial 69...\n",
      "[INFO 08-02 11:12:51] ax.service.managed_loop: Running optimization trial 70...\n",
      "[INFO 08-02 11:12:53] ax.service.managed_loop: Running optimization trial 71...\n",
      "[INFO 08-02 11:12:55] ax.service.managed_loop: Running optimization trial 72...\n",
      "[INFO 08-02 11:12:57] ax.service.managed_loop: Running optimization trial 73...\n",
      "[INFO 08-02 11:12:59] ax.service.managed_loop: Running optimization trial 74...\n",
      "[INFO 08-02 11:13:01] ax.service.managed_loop: Running optimization trial 75...\n",
      "[INFO 08-02 11:13:02] ax.service.managed_loop: Running optimization trial 76...\n",
      "[INFO 08-02 11:13:04] ax.service.managed_loop: Running optimization trial 77...\n",
      "[INFO 08-02 11:13:07] ax.service.managed_loop: Running optimization trial 78...\n",
      "[INFO 08-02 11:13:08] ax.service.managed_loop: Running optimization trial 79...\n",
      "[INFO 08-02 11:13:09] ax.service.managed_loop: Running optimization trial 80...\n",
      "[INFO 08-02 11:13:10] ax.service.managed_loop: Running optimization trial 81...\n",
      "[INFO 08-02 11:13:11] ax.service.managed_loop: Running optimization trial 82...\n",
      "[INFO 08-02 11:13:13] ax.service.managed_loop: Running optimization trial 83...\n",
      "[INFO 08-02 11:13:14] ax.service.managed_loop: Running optimization trial 84...\n",
      "[INFO 08-02 11:13:16] ax.service.managed_loop: Running optimization trial 85...\n",
      "[INFO 08-02 11:13:18] ax.service.managed_loop: Running optimization trial 86...\n",
      "[INFO 08-02 11:13:19] ax.service.managed_loop: Running optimization trial 87...\n",
      "[INFO 08-02 11:13:20] ax.service.managed_loop: Running optimization trial 88...\n",
      "[INFO 08-02 11:13:23] ax.service.managed_loop: Running optimization trial 89...\n",
      "[INFO 08-02 11:13:25] ax.service.managed_loop: Running optimization trial 90...\n",
      "[INFO 08-02 11:13:27] ax.service.managed_loop: Running optimization trial 91...\n",
      "[INFO 08-02 11:13:28] ax.service.managed_loop: Running optimization trial 92...\n",
      "[INFO 08-02 11:13:31] ax.service.managed_loop: Running optimization trial 93...\n",
      "[INFO 08-02 11:13:33] ax.service.managed_loop: Running optimization trial 94...\n",
      "[INFO 08-02 11:13:36] ax.service.managed_loop: Running optimization trial 95...\n",
      "[INFO 08-02 11:13:45] ax.service.managed_loop: Running optimization trial 96...\n",
      "[INFO 08-02 11:13:46] ax.service.managed_loop: Running optimization trial 97...\n",
      "[INFO 08-02 11:13:48] ax.service.managed_loop: Running optimization trial 98...\n",
      "[INFO 08-02 11:13:49] ax.service.managed_loop: Running optimization trial 99...\n",
      "[INFO 08-02 11:13:51] ax.service.managed_loop: Running optimization trial 100...\n"
     ]
    }
   ],
   "source": [
    "def evaluation_function(parameterization):\n",
    "    # Convert parameterization to a DataFrame\n",
    "    param_df = pd.DataFrame([parameterization])\n",
    "    \n",
    "    # Compute the distance between the parameterization and all rows in the test set\n",
    "    distances = euclidean_distances(param_df, X)\n",
    "    \n",
    "    # Find the index of the closest row\n",
    "    closest_index = np.argmin(distances)\n",
    "    \n",
    "    # Get the corresponding output value\n",
    "    output_value = y.iloc[closest_index]\n",
    "    \n",
    "    # Return the output value as the objective\n",
    "    return {\"objective_value\": output_value}\n",
    "\n",
    "# Define the parameters to optimize\n",
    "parameters = [\n",
    "    {\"name\": \"QAgNO3(%)\", \"type\": \"range\", \"bounds\": [float(X['QAgNO3(%)'].min()), float(X['QAgNO3(%)'].max())]},\n",
    "    {\"name\": \"Qpva(%)\", \"type\": \"range\", \"bounds\": [float(X['Qpva(%)'].min()), float(X['Qpva(%)'].max())]},\n",
    "    {\"name\": \"Qtsc(%)\", \"type\": \"range\", \"bounds\": [float(X['Qtsc(%)'].min()), float(X['Qtsc(%)'].max())]},\n",
    "    {\"name\": \"Qseed(%)\", \"type\": \"range\", \"bounds\": [float(X['Qseed(%)'].min()), float(X['Qseed(%)'].max())]},\n",
    "    {\"name\": \"Qtot(uL/min)\", \"type\": \"range\", \"bounds\": [float(X['Qtot(uL/min)'].min()), float(X['Qtot(uL/min)'].max())]}\n",
    "]\n",
    "\n",
    "# Run the optimization\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=parameters,\n",
    "    evaluation_function=evaluation_function,\n",
    "    objective_name='objective_value',\n",
    "    minimize=True,\n",
    "    total_trials=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796bd41b",
   "metadata": {},
   "source": [
    "Finally, let's see what our best parameters are. Keep in mind this can be influenced by the number of trials that the model is running. If it has a very low amount it may not have the chance to find the optimal parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25856cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'QAgNO3(%)': 39.589096718406005, 'Qpva(%)': 12.344281013175706, 'Qtsc(%)': 7.771738511535432, 'Qseed(%)': 4.189543170093936, 'Qtot(uL/min)': 877.4859159430547}\n",
      "Best Objective Value: {'objective_value': 0.17284820699991588}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", best_parameters)\n",
    "print(\"Best Objective Value:\", values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece76b96",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization with Ax\n",
    "\n",
    "In this section we will train a random forest regressor on predicting the loss value from the input data that we used before. We will then use an Ax model to tune the hyperparameters of this model.\n",
    "\n",
    "First we will import the libraries required. We will be using a random forest model from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1ee3a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ax.service.managed_loop import optimize\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923f2ea",
   "metadata": {},
   "source": [
    "Next we will load the same dataset. Everything here is the same as before except we are going to split it into train/test splits in order to be able to correctly score our random forest regression model. It's important to specify the random_state in order to have reproducability between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "531b87db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAgNO3(%)       0\n",
      "Qpva(%)         0\n",
      "Qtsc(%)         0\n",
      "Qseed(%)        0\n",
      "Qtot(uL/min)    0\n",
      "loss            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('AgNP_dataset.csv')\n",
    "\n",
    "\n",
    "# Check for NaN values\n",
    "print(data.isna().sum())\n",
    "\n",
    "# Drop or fill NaN values\n",
    "data = data.dropna()  # Option 1: Drop rows with NaN values\n",
    "# or\n",
    "data = data.fillna(data.mean())  # Option 2: Fill NaN values with column mean\n",
    "\n",
    "# Define the inputs (X) and the output (y)\n",
    "y = data['loss']\n",
    "X = data.drop('loss', axis=1)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92d190e",
   "metadata": {},
   "source": [
    "Next we will train and score our model. The details are covered in the random_forest notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "933ec231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0038889048983534588\n"
     ]
    }
   ],
   "source": [
    "# Train a RandomForestRegressor model\n",
    "model = RandomForestRegressor(random_state=42, n_estimators=10, max_depth=5, min_samples_split=5, min_samples_leaf=2)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2344d638",
   "metadata": {},
   "source": [
    "We have a pretty good MSE to start! However, we can do better than it. We will now use an Ax model to tune the parameters of this random forest model. Much like the optimization above, we list out the parameters we want to tune. However, our evaluation function trains and fits the random forest model with the found parameters, then uses the MSE score to find the optimal values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b834e9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 08-02 11:13:54] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='n_estimators', parameter_type=INT, range=[10, 200]), RangeParameter(name='max_depth', parameter_type=INT, range=[5, 50]), RangeParameter(name='min_samples_split', parameter_type=INT, range=[2, 20]), RangeParameter(name='min_samples_leaf', parameter_type=INT, range=[1, 10])], parameter_constraints=[]).\n",
      "[INFO 08-02 11:13:54] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.\n",
      "[INFO 08-02 11:13:54] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=4 num_trials=None use_batch_trials=False\n",
      "[INFO 08-02 11:13:54] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=8\n",
      "[INFO 08-02 11:13:54] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=8\n",
      "[INFO 08-02 11:13:54] ax.modelbridge.dispatch_utils: `verbose`, `disable_progbar`, and `jit_compile` are not yet supported when using `choose_generation_strategy` with ModularBoTorchModel, dropping these arguments.\n",
      "[INFO 08-02 11:13:54] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 8 trials, BoTorch for subsequent trials]). Iterations after 8 will take longer to generate due to model-fitting.\n",
      "[INFO 08-02 11:13:54] ax.service.managed_loop: Started full optimization with 30 steps.\n",
      "[INFO 08-02 11:13:54] ax.service.managed_loop: Running optimization trial 1...\n",
      "[INFO 08-02 11:13:54] ax.service.managed_loop: Running optimization trial 2...\n",
      "[INFO 08-02 11:13:54] ax.service.managed_loop: Running optimization trial 3...\n",
      "[INFO 08-02 11:13:55] ax.service.managed_loop: Running optimization trial 4...\n",
      "[INFO 08-02 11:13:55] ax.service.managed_loop: Running optimization trial 5...\n",
      "[INFO 08-02 11:13:55] ax.service.managed_loop: Running optimization trial 6...\n",
      "[INFO 08-02 11:13:55] ax.service.managed_loop: Running optimization trial 7...\n",
      "[INFO 08-02 11:13:55] ax.service.managed_loop: Running optimization trial 8...\n",
      "[INFO 08-02 11:13:56] ax.service.managed_loop: Running optimization trial 9...\n",
      "[INFO 08-02 11:13:56] ax.service.managed_loop: Running optimization trial 10...\n",
      "[INFO 08-02 11:13:57] ax.service.managed_loop: Running optimization trial 11...\n",
      "[INFO 08-02 11:13:57] ax.service.managed_loop: Running optimization trial 12...\n",
      "[INFO 08-02 11:13:58] ax.service.managed_loop: Running optimization trial 13...\n",
      "[INFO 08-02 11:13:58] ax.service.managed_loop: Running optimization trial 14...\n",
      "[INFO 08-02 11:13:59] ax.service.managed_loop: Running optimization trial 15...\n",
      "[INFO 08-02 11:13:59] ax.service.managed_loop: Running optimization trial 16...\n",
      "[INFO 08-02 11:14:00] ax.service.managed_loop: Running optimization trial 17...\n",
      "[INFO 08-02 11:14:01] ax.service.managed_loop: Running optimization trial 18...\n",
      "[INFO 08-02 11:14:01] ax.service.managed_loop: Running optimization trial 19...\n",
      "[INFO 08-02 11:14:02] ax.service.managed_loop: Running optimization trial 20...\n",
      "[INFO 08-02 11:14:02] ax.service.managed_loop: Running optimization trial 21...\n",
      "[INFO 08-02 11:14:03] ax.service.managed_loop: Running optimization trial 22...\n",
      "[INFO 08-02 11:14:04] ax.service.managed_loop: Running optimization trial 23...\n",
      "[INFO 08-02 11:14:05] ax.service.managed_loop: Running optimization trial 24...\n",
      "[INFO 08-02 11:14:06] ax.service.managed_loop: Running optimization trial 25...\n",
      "[INFO 08-02 11:14:07] ax.service.managed_loop: Running optimization trial 26...\n",
      "[INFO 08-02 11:14:08] ax.service.managed_loop: Running optimization trial 27...\n",
      "[INFO 08-02 11:14:09] ax.service.managed_loop: Running optimization trial 28...\n",
      "[INFO 08-02 11:14:10] ax.service.managed_loop: Running optimization trial 29...\n",
      "[INFO 08-02 11:14:10] ax.service.managed_loop: Running optimization trial 30...\n"
     ]
    }
   ],
   "source": [
    "def evaluation_function(parameterization):\n",
    "    # Extract hyperparameters from parameterization\n",
    "    n_estimators = parameterization.get(\"n_estimators\")\n",
    "    max_depth = parameterization.get(\"max_depth\")\n",
    "    min_samples_split = parameterization.get(\"min_samples_split\")\n",
    "    min_samples_leaf = parameterization.get(\"min_samples_leaf\")\n",
    "    \n",
    "    # Train the model with given hyperparameters\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and calculate MSE on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "    # Return the MSE as the objective value to minimize\n",
    "    return {\"objective_value\": mse}\n",
    "\n",
    "\n",
    "\n",
    "# Define the parameters to optimize\n",
    "parameters = [\n",
    "    {\"name\": \"n_estimators\", \"type\": \"range\", \"bounds\": [10, 200], \"value_type\": \"int\"},\n",
    "    {\"name\": \"max_depth\", \"type\": \"range\", \"bounds\": [5, 50], \"value_type\": \"int\"},\n",
    "    {\"name\": \"min_samples_split\", \"type\": \"range\", \"bounds\": [2, 20], \"value_type\": \"int\"},\n",
    "    {\"name\": \"min_samples_leaf\", \"type\": \"range\", \"bounds\": [1, 10], \"value_type\": \"int\"}\n",
    "]\n",
    "\n",
    "# Run the optimization\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=parameters,\n",
    "    evaluation_function=evaluation_function,\n",
    "    objective_name='objective_value',\n",
    "    minimize=True,\n",
    "    total_trials=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f6b6d5",
   "metadata": {},
   "source": [
    "Now lets check our parameters and objective value. As you can see, the MSE score went down by a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "921febb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 50, 'max_depth': 17, 'min_samples_split': 20, 'min_samples_leaf': 3}\n",
      "Original Score: 0.0038889048983534588\n",
      "Hyperparameter Tuned Score {'objective_value': 0.002007432932232618}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", best_parameters)\n",
    "print(\"Original Score:\", mse)\n",
    "print(\"Hyperparameter Tuned Score\", values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f86161",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we used Bayesian Optimization with the Ax library to efficiently find the minimum of a synthetic function, optimize within a dataset, and perform hyperparameter tuning on another model. Ax facilitated the process of defining the experiment, running the optimization, and analyzing the results. \n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Ax Documentation](https://ax.dev/)\n",
    "- [More on Bayesian Optimization](https://arxiv.org/abs/1807.02811)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
