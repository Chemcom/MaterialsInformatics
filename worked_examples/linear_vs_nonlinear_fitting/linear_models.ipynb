{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Example\n",
    "\n",
    "#### Video\n",
    "\n",
    "https://www.youtube.com/watch?v=YgaBv6CXfvo&list=PLL0SWcFqypCl4lrzk1dMWwTUrzQZFt7y0&index=16 (Linear vs Nonlinear models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Needed\n",
    "- pandas\n",
    "- sklearn\n",
    "- CBFV\n",
    "- matplotlib\n",
    "- time\n",
    "- collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from CBFV import composition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from collections import OrderedDict\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "After loading the dataset into a dataframe and extracting the wanted columns, the data is turned into a composition based feature vector using the CBFV library. The standardization and normalization of the data makes sure that all the data is on the same scale (if you have a column with very small numbers and a column with a larger magnitude it can make a model perform worse). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and name it for use with CBFV\n",
    "df_train = read_csv('cp_train.csv')\n",
    "df_val = read_csv('cp_val.csv')\n",
    "df_test = read_csv('cp_test.csv')\n",
    "\n",
    "# Split the dataset into testing, training, and validation sets\n",
    "rename_dict = {'Cp': 'target'}\n",
    "df_train = df_train.rename(columns=rename_dict) # Dataset to train the model on\n",
    "df_val = df_val.rename(columns=rename_dict) # Dataset to test the model against in order to tune it \n",
    "df_test = df_test.rename(columns=rename_dict) # Dataset to test the model against as the last step\n",
    "\n",
    "# Create the CBFV from your loaded dataset\n",
    "X_train, y_train, formulae, skipped = composition.generate_features(df_train, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "X_val, y_val, formulae, skipped = composition.generate_features(df_val, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "X_test, y_test, formulae, skipped = composition.generate_features(df_test, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "\n",
    "# Scale and normalize the data AFTER transforming it into a CBFV \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train = normalize(X_train_scaled)\n",
    "X_val = normalize(X_val_scaled)\n",
    "X_test = normalize(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge\n",
    "\n",
    "The first model we will look at is a Ridge Regression model. Ridge models are good for datasets where there might be correlated features. In this instance we will use it to find out what features may be of any significance in predicting the target value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model with no hyperparameter tuning for now\n",
    "model = Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, random_state=None, solver='auto', tol=0.001)\n",
    "\n",
    "# Train the Ridge regression model on the training dataset\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained model to make predictions on the validation dataset\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Evaluate and collect performance metrics for the model\n",
    "r2 = r2_score(y_val, y_pred)  # Coefficient of determination (R^2)\n",
    "mae = mean_absolute_error(y_val, y_pred)  # Mean Absolute Error (MAE)\n",
    "rmse_val = mean_squared_error(y_val, y_pred, squared=False)  # Root Mean Squared Error (RMSE)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(f'R^2 on val set is {r2:.2f}')\n",
    "print(f'MAE on val set is {mae:.2f}')\n",
    "print(f'RMSE on val set is {rmse_val:.2f}')\n",
    "\n",
    "# Calculate and report feature importance\n",
    "importance = model.coef_\n",
    "\n",
    "# Visualize the feature importance using a bar plot\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "\n",
    "Lasso Regression models are very similar to Ridge Regression models in that they are good for dealing with datasets where the features may have correlatino to the target value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Lasso regression model with no hyperparameter tuning\n",
    "model = Lasso(alpha=1.0)\n",
    "\n",
    "# Train the Lasso regression model on the training dataset\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained model to make predictions on the validation dataset\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Evaluate and collect performance metrics for the model\n",
    "r2 = r2_score(y_val, y_pred)  # Coefficient of determination (R^2)\n",
    "mae = mean_absolute_error(y_val, y_pred)  # Mean Absolute Error (MAE)\n",
    "rmse_val = mean_squared_error(y_val, y_pred, squared=False)  # Root Mean Squared Error (RMSE)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(f'R^2 on val set is {r2:.2f}')\n",
    "print(f'MAE on val set is {mae:.2f}')\n",
    "print(f'RMSE on val set is {rmse_val:.2f}')\n",
    "\n",
    "# Calculate and report feature importance (coefficients) from the trained model\n",
    "importance = model.coef_\n",
    "\n",
    "# Visualize the feature importance using a bar plot\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (MaterialsInformatics)",
   "language": "python",
   "name": "materialsinformatics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
