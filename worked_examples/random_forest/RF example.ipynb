{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Example\n",
    "For this notebook I'll be pulling some data from Materials Project. I'll use the old api using my MyPymatgen virtual environment\n",
    "\n",
    "Let's start by getting our API key loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymatgen.ext.matproj import MPRester\n",
    "import os\n",
    "\n",
    "filename = r'G:\\My Drive\\teaching\\5540-6640 Materials Informatics\\old_apikey.txt'\n",
    "\n",
    "def get_file_contents(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            # It's assumed our file contains a single line,\n",
    "            # with our API key\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(\"'%s' file not found\" % filename)\n",
    "\n",
    "\n",
    "Sparks_API = get_file_contents(filename)\n",
    "mpr = MPRester(Sparks_API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab some data to work with. We'll grab chlorides within 1 meV of the convex hull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=('pretty_formula', 'band_gap',\n",
    "                           \"density\", 'formation_energy_per_atom', 'volume'))\n",
    "\n",
    "# grab some props for stable chlorides\n",
    "criteria = {'e_above_hull': {'$lte': 0.001},'elements':{'$all':['Cl']}}\n",
    "# criteria2 = {'e_above_hull': {'$lte': 0.02},'elements':{'$all':['O']},\n",
    "#              'band_gap':{'$ne':0}}\n",
    "\n",
    "props = ['pretty_formula', 'band_gap', \"density\",\n",
    "         'formation_energy_per_atom', 'volume']\n",
    "entries = mpr.query(criteria=criteria, properties=props)\n",
    "\n",
    "i = 0\n",
    "for entry in entries:\n",
    "    df.loc[i] = [entry['pretty_formula'], entry['band_gap'], entry['density'],\n",
    "                 entry['formation_energy_per_atom'], entry['volume']]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "RNG_SEED = 42\n",
    "np.random.seed(seed=RNG_SEED)\n",
    "\n",
    "X = df[['band_gap','formation_energy_per_atom','volume']]\n",
    "y = df['density']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RNG_SEED)\n",
    "rf = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('the r2 score is',r2)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('the mean absolute error is',mae)\n",
    "rmse_val = mean_squared_error(y_test, y_pred, squared=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model isn't too great alone, but what if we add CBFV features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CBFV import composition\n",
    "import time\n",
    "\n",
    "rename_dict = {'density': 'target', 'pretty_formula':'formula'}\n",
    "df = df.rename(columns=rename_dict)\n",
    "\n",
    "\n",
    "RNG_SEED = 42\n",
    "np.random.seed(seed=RNG_SEED)\n",
    "\n",
    "X = df[['formula','band_gap','formation_energy_per_atom','volume']]\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RNG_SEED)\n",
    "\n",
    "X_train, y_train, formulae_train, skipped_train = composition.generate_features(df, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "X_test, y_test, formulae_train, skipped_train = composition.generate_features(df, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "\n",
    "\n",
    "#technically we should scale and normalize our data here... but lets skip it for now\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate the training time\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "rf = RandomForestRegressor(max_depth=4, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('the r2 score is',r2)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('the mean absolute error is',mae)\n",
    "rmse_val = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"Training time:\", training_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way better! Our R^2 went way up, and our MAE went way down.\n",
    "\n",
    "# Grid Search Hyperparameter Tuning\n",
    "\n",
    "Now let's try one more time, but this time we'll do hyperparameter tuning!\n",
    "Note- we're going to reduce our data down to just 300 points during hyperparameter tuning or it will take foreeeeeeevvvvveeerrr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "# Select a subset of the dataframe with 300 data points\n",
    "subset_df = df.sample(n=300, random_state=RNG_SEED)\n",
    "\n",
    "# Split the subset into training and testing sets\n",
    "rename_dict = {'density': 'target', 'pretty_formula':'formula'}\n",
    "subset_df = subset_df.rename(columns=rename_dict)\n",
    "RNG_SEED = 42\n",
    "np.random.seed(seed=RNG_SEED)\n",
    "X = subset_df[['formula','band_gap','formation_energy_per_atom','volume']]\n",
    "y = subset_df['target']\n",
    "\n",
    "#now do CBFV\n",
    "X_train, y_train, formulae_train, skipped_train = composition.generate_features(subset_df, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "X_test, y_test, formulae_train, skipped_train = composition.generate_features(subset_df, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Create the random forest regressor\n",
    "rf_subset = RandomForestRegressor(random_state=RNG_SEED)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search_subset = GridSearchCV(estimator=rf_subset, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Start the timer\n",
    "start_time_subset = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search_subset.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the training time\n",
    "training_time_subset = time.time() - start_time_subset\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_subset = grid_search_subset.best_params_\n",
    "best_score_subset = grid_search_subset.best_score_\n",
    "\n",
    "# Train the model with the best parameters\n",
    "rf_best_subset_grid = RandomForestRegressor(random_state=RNG_SEED, **best_params_subset)\n",
    "rf_best_subset_grid.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_subset = rf_best_subset_grid.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2_subset = r2_score(y_test, y_pred_subset)\n",
    "mae_subset = mean_absolute_error(y_test, y_pred_subset)\n",
    "rmse_subset = mean_squared_error(y_test, y_pred_subset, squared=False)\n",
    "\n",
    "print(\"Best parameters (subset):\", best_params_subset)\n",
    "print(\"Best score (subset):\", best_score_subset)\n",
    "print(\"R2 score (subset):\", r2_subset)\n",
    "print(\"Mean absolute error (subset):\", mae_subset)\n",
    "print(\"Root mean squared error (subset):\", rmse_subset)\n",
    "print(\"Training time (subset):\", training_time_subset, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search hyperparameter tuning\n",
    "Now let's try random search hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Create the random forest regressor\n",
    "rf_subset = RandomForestRegressor(random_state=0)\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(estimator=rf_subset, param_distributions=param_grid, n_iter=10, cv=5, random_state=0)\n",
    "\n",
    "# Start the timer\n",
    "start_time_subset = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search_subset.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the training time\n",
    "training_time_subset = time.time() - start_time_subset\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_subset = grid_search_subset.best_params_\n",
    "best_score_subset = grid_search_subset.best_score_\n",
    "\n",
    "# Train the model with the best parameters\n",
    "rf_best_subset_random = RandomForestRegressor(random_state=RNG_SEED, **best_params_subset)\n",
    "rf_best_subset_random.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_subset = rf_best_subset_random.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2_subset = r2_score(y_test, y_pred_subset)\n",
    "mae_subset = mean_absolute_error(y_test, y_pred_subset)\n",
    "rmse_subset = mean_squared_error(y_test, y_pred_subset, squared=False)\n",
    "\n",
    "print(\"Best parameters (subset):\", best_params_subset)\n",
    "print(\"Best score (subset):\", best_score_subset)\n",
    "print(\"R2 score (subset):\", r2_subset)\n",
    "print(\"Mean absolute error (subset):\", mae_subset)\n",
    "print(\"Root mean squared error (subset):\", rmse_subset)\n",
    "print(\"Training time (subset):\", training_time_subset, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#featurize with all the data, not just the subset\n",
    "X_train, y_train, formulae_train, skipped_train = composition.generate_features(df, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "X_test, y_test, formulae_train, skipped_train = composition.generate_features(df, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model with the best parameters\n",
    "rf_best_subset_random.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the training time\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = rf_best_subset_random.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(\"R2 score:\", r2)\n",
    "print(\"Mean absolute error:\", mae)\n",
    "print(\"Root mean squared error:\", rmse)\n",
    "print(\"Training time:\", training_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree visualization\n",
    "Finally, we can do tree visualization with graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "# Pull out one tree from the forest\n",
    "tree = rf.estimators_[5]\n",
    "# Import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "# Pull out one tree from the forest\n",
    "tree = rf.estimators_[5]\n",
    "# Export the image to a dot file\n",
    "feature_list = list(X_train.columns)\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n",
    "# Use dot file to create a graph\n",
    "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "# Write graph to a png file\n",
    "graph.write_png('tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost and others\n",
    "\n",
    "There are many other variants of random forests. Adaboost, XGBoost etc. XGBoost, in particular, has emerged as one of the favorite and best classical models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it yourself!\n",
    "\n",
    "- Find oxides with MPRester and load their formula and band gap energy\n",
    "- Create a CBFV with this data for use in training models \n",
    "- Create a RF model using this data and score it \n",
    "- Create a single decision tree model (using sklearn DecisionTreeClassifier) using this data and score it\n",
    "- How do the scores of the two models compare? Why? \n",
    "- Extract the feature importance from the RF model and see which features matter the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
