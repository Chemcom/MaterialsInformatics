{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95152079",
   "metadata": {},
   "source": [
    "# Introduction to Bayesian Optimization\n",
    "\n",
    "In this notebook, we'll explore the basics of Bayesian Optimization, a powerful method for optimizing black-box functions. We'll implement a simple Bayesian Optimization algorithm from scratch using basic libraries like numpy and pandas.\n",
    "\n",
    "## Key Components\n",
    "- **Objective Function**: The function we want to optimize.\n",
    "- **Surrogate Model**: A model that approximates the objective function.\n",
    "- **Acquisition Function**: A function that guides the search for the optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d49c1",
   "metadata": {},
   "source": [
    "# Setting Up the Environment\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up some helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde7a84",
   "metadata": {},
   "source": [
    "# Define the Objective Function\n",
    "\n",
    "We'll define a simple objective function to optimize. For demonstration purposes, we'll use the function \\( f(x) = -x^2 + 4 \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192cd851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective_function(x):\n",
    "    return -x**2 + 4\n",
    "\n",
    "# Example usage\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = objective_function(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title('Objective Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e8f647",
   "metadata": {},
   "source": [
    "# Surrogate Model with Gaussian Process\n",
    "\n",
    "We'll use a Gaussian Process (GP) as our surrogate model. The GP will help us model the objective function and make predictions about its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d832ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gaussian_kernel(x1, x2, length_scale=1.0, sigma_f=1.0):\n",
    "    \"\"\"\n",
    "    Squared Exponential Kernel (Gaussian Kernel)\n",
    "    \"\"\"\n",
    "    sqdist = np.sum((x1 - x2)**2)\n",
    "    return sigma_f**2 * np.exp(-0.5 / length_scale**2 * sqdist)\n",
    "\n",
    "# Example usage\n",
    "x1, x2 = np.array([1]), np.array([2])\n",
    "print(f\"Gaussian Kernel between {x1} and {x2}: {gaussian_kernel(x1, x2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f177c",
   "metadata": {},
   "source": [
    "# Acquisition Function\n",
    "\n",
    "The acquisition function helps us decide where to sample next. One popular choice is the Expected Improvement (EI) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7a204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def expected_improvement(x, x_sample, y_sample, gp, xi=0.01):\n",
    "    \"\"\"\n",
    "    Computes the EI at points x based on existing samples x_sample and y_sample\n",
    "    using a Gaussian Process surrogate model.\n",
    "    \"\"\"\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu_sample_opt = np.max(y_sample)\n",
    "\n",
    "    with np.errstate(divide='warn'):\n",
    "        imp = mu - mu_sample_opt - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "\n",
    "    return ei\n",
    "\n",
    "# Example usage\n",
    "# To be filled during the live coding session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac26c5",
   "metadata": {},
   "source": [
    "# Bayesian Optimization Loop\n",
    "\n",
    "In this section, we'll implement the main loop of Bayesian Optimization, which includes updating the surrogate model and optimizing the acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada3e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bayesian_optimization(n_iters, sample_loss, bounds, x0=None):\n",
    "    x_sample = np.array(x0)\n",
    "    y_sample = sample_loss(x_sample)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        # Update the Gaussian Process with new samples\n",
    "        gp.fit(x_sample, y_sample)\n",
    "        \n",
    "        # Propose the next sampling point by optimizing the acquisition function\n",
    "        x_next = propose_location(expected_improvement, x_sample, y_sample, gp, bounds)\n",
    "        \n",
    "        # Obtain the next sample from the objective function\n",
    "        y_next = sample_loss(x_next)\n",
    "        \n",
    "        # Append new sample to previous samples\n",
    "        x_sample = np.vstack((x_sample, x_next))\n",
    "        y_sample = np.vstack((y_sample, y_next))\n",
    "        \n",
    "    return x_sample, y_sample\n",
    "\n",
    "# Example usage\n",
    "# To be filled during the live coding session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4394ce92",
   "metadata": {},
   "source": [
    "# Visualization and Analysis\n",
    "\n",
    "Let's visualize the results of our optimization process, including the objective function, surrogate model, and acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde203eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting functions\n",
    "def plot_gp(gp, x, y, x_sample, y_sample, x_next=None):\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    plt.plot(x, y, 'r:', label='Objective Function')\n",
    "    plt.plot(x, mu, 'b-', label='GP Mean')\n",
    "    plt.fill_between(x.ravel(), mu - 1.96*sigma, mu + 1.96*sigma, alpha=0.2, color='k')\n",
    "    plt.plot(x_sample, y_sample, 'r.', markersize=10, label='Samples')\n",
    "    if x_next is not None:\n",
    "        plt.axvline(x_next, color='k', linestyle='--', label='Next Sample')\n",
    "    plt.legend()\n",
    "    plt.title('Gaussian Process and Objective Function')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# To be filled during the live coding session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9a47ef",
   "metadata": {},
   "source": [
    "# Real-World Application in Materials Informatics\n",
    "\n",
    "Bayesian Optimization can be used in materials informatics for tasks such as optimizing material properties. For instance, it can help in discovering materials with desired characteristics by efficiently exploring the space of material compositions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ff637",
   "metadata": {},
   "source": [
    "# Q&A and Further Reading\n",
    "\n",
    "Feel free to ask any questions! For further reading, check out the following resources:\n",
    "- [Bayesian Optimization Blog](https://blog.alan.dev/bayesian-optimization)\n",
    "- [Gaussian Processes for Machine Learning](http://www.gaussianprocess.org/gpml/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10897d",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we've built a simple Bayesian Optimization algorithm from scratch using basic libraries. We've covered the essential components, including the objective function, surrogate model, and acquisition function. Bayesian Optimization is a powerful tool for optimizing expensive and complex functions, and we encourage you to explore it further."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
