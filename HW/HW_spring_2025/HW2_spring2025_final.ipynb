{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import seaborn as sns\n",
    "import os \n",
    "\n",
    "\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "from sklearn.svm import SVR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import the smiles_tg.csv file into a dataframe and identify how many unique smiles strings are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop the duplicate smiles strings in the dataset, keep the first entry and reset the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we are going to use the RDKit library to convert the smiles strings into molecular objects\n",
    "### that we will use as features for our models\n",
    "### lets first generate basic descriptors for the molecules\n",
    "\n",
    "### here you will write a basic function called get_basic_descriptors, that takes a smiles string as input, and returns a dictionary of descriptors\n",
    "### the descriptors are the following:\n",
    "### 'MW': molecular weight\n",
    "### 'HBD': number of hydrogen bond donors\n",
    "### 'HBA': number of hydrogen bond acceptors\n",
    "### 'TPSA': topological polar surface area\n",
    "### 'Rotatable_Bonds': number of rotatable bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next create a function called get_morgan_fingerprint that takes in smiles strings and generates morgan fingerprints\n",
    "### with a radius of 2 and a length nBits= 1024 \n",
    "### Return the fingerprint as a list\n",
    "\n",
    "### hint: the Input parameters to the function should be\n",
    "### smiles, radius, nBits\n",
    "### the function should return the fingerprint as a list\n",
    "### this can be done in a very few lines of code (don't complicate it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next create a function called get_topological_fingerprint that generates topological fingerprints from SMILES strings. These fingerprints capture the 2D structural features of molecules. \n",
    "### The function should take in a smiles string and nBits=25\n",
    "### return the fingerprint as a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we are now going to use these functions to generate features for our models.\n",
    "### lets start with the get_basic_descriptors function, use it to convert the smiles strings in the dataset to features\n",
    "### so what you will do here is create a list that contains the descriptors for each smiles string in the dataset\n",
    "### remember, in your function, this should return a dictionary of descriptors, so you will have a single list, where each smile string is represented by a dictionary of descriptors\n",
    "\n",
    "### if you did this correctly and print the output, the first entry should look similar to this:\n",
    "\n",
    "### [{'MW': 167.188, 'HBD': 0, 'HBA': 5, 'TPSA': 75.99, 'Rotatable_Bonds': 0},{.....}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert this list of dictionaries into a dataframe called df_descriptors\n",
    "### next lets scale the features using the StandardScaler from sklearn.preprocessing\n",
    "### fit and transform the dataframe and we'll call this scaled dataframe X\n",
    "### now that your data is transformed, use KMeans clustering to cluster and fit the scaled dataframe (X).\n",
    "### use n_clusters = 5, and a random_state = 0\n",
    "### extract the labels using kmeans.labels_ and add this as a column a new column in the df_descriptors called 'cluster'\n",
    "### next use the PCA algorithmn to reduce the dimensionality of the dataframe (X) to 2 dimensions\n",
    "### Add the the pca1 and pca2 columns to the df_descriptors dataframe\n",
    "### finally, use the seaborn library to create a scatter plot \n",
    "### set data = descriptors, x = pca1, y = pca2, hue = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next lets explore UMAP. UMAP is a dimensionality reduction technique that is used for visualization of high-dimensional data\n",
    "### take the list of dictionaries we created earlier, and create another dataframe from it.\n",
    "### use this new dataframe to create a UMAP plot with 2 dimensions\n",
    "### set the umap parameters to n_neighbors = 15, min_dist = 0.1, n_components = 2\n",
    "### when making the umap plot, set a color bar as the Tg values from the original dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next, create a train test split of the data, using the featurized dataset and the Tg values\n",
    "### set the test_size =0.3 and a random_state = 42\n",
    "### traina linear model (ridge regression) and a non-linear model (random forest) on the training data\n",
    "### test the mdoels on the test set and print the r2 and rmse score for each model\n",
    "### create 2 parity plots, one for each model\n",
    "### In either of these parity plots, can you spot a clear outlier in the model?\n",
    "### if we were to remove this outlier, would our model improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next, lets see if using gridsearchcv can improve the performance of the models.\n",
    "### repeat the above excerise, but this time use gridsearchcv to find the best hyperparameters for the models\n",
    "### for the ridge model set the param_grid for alpha to alpha = [0.01, 0.1, 1.0, 10, 100], set cv =5, scoring= r^2 and n_jobs = -1.\n",
    "### For the random forest model use this param_grid to search through param_grid_rf = {\n",
    "###    'n_estimators': [50, 100, 200],\n",
    "###    'max_depth': [None, 5, 10],}\n",
    "### print the best score and the best parameters for each model\n",
    "### create a model from the best parameters and test the model on the test set\n",
    "### do you see an improvement in the performance of the models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next, lets generate features from the get_morgan_fingerprint function\n",
    "### create one list called morgan_fingerprints of the morgan fingerprints for each smiles string in the dataset\n",
    "### next, create a new dataframe from the morgan_fingerprints list\n",
    "### next, split the data set into a test and train set using the Tg values as your target variable,\n",
    "### set the test_size = 0.3 and random_state = 42\n",
    "### standardize the data using the StandardScaler, and train a linear ridge(alpha=0.1) model and a non-linear random forest(n_estimators=100,random_state=42) model\n",
    "### test the models on the test set and print the r2 and rmse score for each model\n",
    "### plot a parity plot for each model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next lets use the get_topological_fingerprint function to generate 2D structural features of our smiles strings\n",
    "### create a list called topological_fps of the topological fingerprints for each smiles string in the dataset\n",
    "### and convert this to a dataframe. Create a train test split of the data using the Tg values as the target variable\n",
    "### set the test_size = 0.3 and random_state = 42. \n",
    "### for this excercise we are going to train a Support vector regression and a random forest model.\n",
    "### We will also want to create a 3rd model called ensemble made up of both of the models.\n",
    "### using the VotingRegressor from sklearn.ensemble, create a model that combines the SVR and random forest models.\n",
    "### test each model, print the r2 and rmse score for each all 3 models as well as a parity plot for each model.\n",
    "### Do we see any improvement in the performance of the models when we combine into an ensemble of models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for the final excercise, we are going to identify the most similar molecules in our dataset relative to the first molecule in the dataset.\n",
    "### so we want to compare how similar the first molecule is to the rest of the molecules in the dataset\n",
    "### generate morgan fingerprints for the first molecule in the dataset and compare it to the rest of the molecules in the dataset\n",
    "### you'll first generate morgan fingerprints(radius=2, nBits=1024) for the first molecule in the dataset\n",
    "### and then for the rest of the molecules in the dataset. Calulate the Tanimoto similarity between the first molecule and the rest of the molecules in the dataset, print the top 10 most similar molecules to the first molecule in the dataset\n",
    "### it should look something like this:\n",
    "\n",
    "### Most similar molecules:\n",
    "###                             SMILES                                                     Similarity\n",
    "### 0    *C1COC2C1OCC2Oc1ccc(cc1)CNC(=O)CCCCCCC(=O)NCc1...                                  1.000000\n",
    "### 222                 *N=Occ1                                                             0.444444\n",
    "### 333  *Oc1cCC(=O)NCCCc1cccOC...                                                          0.428571\n",
    "### 444  *C1C2OC(=O)CCC(=O)O                                                                0.428571\n",
    "### 20   *NC....                                                                            0.425926\n",
    "### 12   *OC(=O)CNC...                                                                      0.353846\n",
    "\n",
    "### use MolsToGridImage to display the molecule and the 4 most similar molecules to the first molecule in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
