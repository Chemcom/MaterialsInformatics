{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8178ad1",
   "metadata": {},
   "source": [
    "# Bayesian Optimization with Ax Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use Bayesian Optimization with the Ax library to optimize machine learning model parameters and other experiments efficiently. Ax is a powerful tool developed by Facebook for automating experimentation and optimization.\n",
    "\n",
    "## What is Bayesian Optimization?\n",
    "\n",
    "Bayesian Optimization is an efficient strategy for optimizing black-box functions that are expensive to evaluate. It's particularly well-suited for high-dimensional spaces and situations where sampling data is costly.\n",
    "\n",
    "## Introduction to Ax\n",
    "\n",
    "Ax is an accessible, modular, and efficient library that supports both gradient-based optimization and Bayesian Optimization. It is designed to automate the process of optimizing complex experiments, like tuning hyperparameters for machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa56809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "!pip install ax-platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d257042e",
   "metadata": {},
   "source": [
    "# Basic Ax Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407d5b7",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "After installing Ax, we need to import the necessary libraries to set up our optimization problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ea947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Ax libraries\n",
    "from ax.service.ax_client import AxClient\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc5ac43",
   "metadata": {},
   "source": [
    "## Defining the Optimization Problem\n",
    "\n",
    "For this tutorial, we'll optimize a simple synthetic function as our objective. Our goal is to find the minimum value of this function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3520065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective_function(parameters):\n",
    "    x = parameters.get(\"x\")\n",
    "    y = parameters.get(\"y\")\n",
    "    objective_value = (x - 0.5)**2 + (y - 0.5)**2  # Simple convex function\n",
    "    return {\"objective_value\": (objective_value, 0.0)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7088a4bd",
   "metadata": {},
   "source": [
    "## Configuring Bayesian Optimization in Ax\n",
    "\n",
    "We will now configure the Bayesian Optimization process in Ax by setting up an experiment. The evaluation function calculates an objective value based on the parameters x and y. We are using a simple quadratic function for this problem. This function matches our originally defined objective function. The optimize function takes a few inputs. The first is a list of parameters to be optimized. Each is given a name, type, and optimization bounds. Next, you have to define an evaluation function for the model to use to evaluate the generated parameters. Next is the objective to be optimized. This is the name of the value. This needs to match the name defined in the objective function. Next, tell the model if it's trying to minimize or maximize. Lastly you can specify the number of trials it needs to run. The higher this number is the more accurate it will be. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a90ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct way to initialize AxClient for Bayesian Optimization with objective setup\n",
    "from ax.service.managed_loop import optimize\n",
    "\n",
    "def evaluation_function(parameterization):\n",
    "    x = parameterization.get(\"x\")\n",
    "    y = parameterization.get(\"y\")\n",
    "    return (x - 0.5)**2 + (y - 0.5)**2  # Example objective function\n",
    "\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=[\n",
    "        {\"name\": \"x\", \"type\": \"range\", \"bounds\": [0.0, 1.0]},\n",
    "        {\"name\": \"y\", \"type\": \"range\", \"bounds\": [0.0, 1.0]},\n",
    "    ],\n",
    "    evaluation_function=evaluation_function,\n",
    "    objective_name='objective_value',\n",
    "    minimize=True,  # Specifies that we are minimizing our objective\n",
    "    total_trials=15\n",
    ")\n",
    "\n",
    "print(\"Best Parameters:\", best_parameters)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d31528",
   "metadata": {},
   "source": [
    "## Using Ax to Optimize Within a Dataset\n",
    "\n",
    "First we need to import our libraries. The biggest new one that we need is the euclidean_distances. This is useful when trying to optimize from values within a dataset. It gives the model an 'evaluation function' to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45419177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from ax.service.managed_loop import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f466b3d8",
   "metadata": {},
   "source": [
    "Next we need to import and clean up our data. It's important to do this step as it can cause the Ax model to run into errors down the line or be unable to efficiently find the optimal value. Let's inspect our data and see what we're working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce34d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('AgNP_dataset.csv')\n",
    "\n",
    "# Drop or fill NaN values\n",
    "data = data.dropna()  # Option 1: Drop rows with NaN values\n",
    "# or\n",
    "data = data.fillna(data.mean())  # Option 2: Fill NaN values with column mean\n",
    "\n",
    "# Define the inputs (X) and the output (y)\n",
    "y = data['loss']\n",
    "X = data.drop('loss', axis=1)\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b128e",
   "metadata": {},
   "source": [
    "3295 is a great amount of data to be working with. We can see that our loss varies bewteen .131 and .910\n",
    "\n",
    "Next we need to create our model and test it. The optimize function uses very similar inputs as the model above, except this time it has a lot more parameters to optimize than before. Since we are optimizing within a dataset we can set the bounds to be the max and min of each input column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa7db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_function(parameterization):\n",
    "    # Convert parameterization to a DataFrame\n",
    "    param_df = pd.DataFrame([parameterization])\n",
    "    \n",
    "    # Compute the distance between the parameterization and all rows in the test set\n",
    "    distances = euclidean_distances(param_df, X)\n",
    "    \n",
    "    # Find the index of the closest row\n",
    "    closest_index = np.argmin(distances)\n",
    "    \n",
    "    # Get the corresponding output value\n",
    "    output_value = y.iloc[closest_index]\n",
    "    \n",
    "    # Return the output value as the objective\n",
    "    return {\"objective_value\": output_value}\n",
    "\n",
    "# Define the parameters to optimize\n",
    "parameters = [\n",
    "    {\"name\": \"QAgNO3(%)\", \"type\": \"range\", \"bounds\": [X['QAgNO3(%)'].min(), X['QAgNO3(%)'].max()]},\n",
    "    {\"name\": \"Qpva(%)\", \"type\": \"range\", \"bounds\": [X['Qpva(%)'].min(), X['Qpva(%)'].max()]},\n",
    "    {\"name\": \"Qtsc(%)\", \"type\": \"range\", \"bounds\": [X['Qtsc(%)'].min(), X['Qtsc(%)'].max()]},\n",
    "    {\"name\": \"Qseed(%)\", \"type\": \"range\", \"bounds\": [X['Qseed(%)'].min(), X['Qseed(%)'].max()]},\n",
    "    {\"name\": \"Qtot(uL/min)\", \"type\": \"range\", \"bounds\": [X['Qtot(uL/min)'].min(), X['Qtot(uL/min)'].max()]}\n",
    "]\n",
    "\n",
    "# Run the optimization\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=parameters,\n",
    "    evaluation_function=evaluation_function,\n",
    "    objective_name='objective_value',\n",
    "    minimize=True,\n",
    "    total_trials=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796bd41b",
   "metadata": {},
   "source": [
    "Finally, let's see what our best parameters are. Keep in mind this can be influenced by the number of trials that the model is running. If it has a very low amount it may not have the chance to find the optimal parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25856cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters:\", best_parameters)\n",
    "print(\"Best Objective Value:\", values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece76b96",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization with Ax\n",
    "\n",
    "In this section we will train a random forest regressor on predicting the loss value from the input data that we used before. We will then use an Ax model to tune the hyperparameters of this model.\n",
    "\n",
    "First we will import the libraries required. We will be using a random forest model from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ee3a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ax.service.managed_loop import optimize\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923f2ea",
   "metadata": {},
   "source": [
    "Next we will load the same dataset. Everything here is the same as before except we are going to split it into train/test splits in order to be able to correctly score our random forest regression model. It's important to specify the random_state in order to have reproducability between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b87db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('AgNP_dataset.csv')\n",
    "\n",
    "\n",
    "# Check for NaN values\n",
    "print(data.isna().sum())\n",
    "\n",
    "# Drop or fill NaN values\n",
    "data = data.dropna()  # Option 1: Drop rows with NaN values\n",
    "# or\n",
    "data = data.fillna(data.mean())  # Option 2: Fill NaN values with column mean\n",
    "\n",
    "# Define the inputs (X) and the output (y)\n",
    "y = data['loss']\n",
    "X = data.drop('loss', axis=1)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92d190e",
   "metadata": {},
   "source": [
    "Next we will train and score our model. The details are covered in the random_forest notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933ec231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForestRegressor model\n",
    "model = RandomForestRegressor(random_state=42, n_estimators=10, max_depth=5, min_samples_split=5, min_samples_leaf=2)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2344d638",
   "metadata": {},
   "source": [
    "We have a pretty good MSE to start! However, we can do better than it. We will now use an Ax model to tune the parameters of this random forest model. Much like the optimization above, we list out the parameters we want to tune. however, our evaluation function trains and fits the random forest model with the found parameters, then uses the MSE score to find the optimal values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_function(parameterization):\n",
    "    # Extract hyperparameters from parameterization\n",
    "    n_estimators = parameterization.get(\"n_estimators\")\n",
    "    max_depth = parameterization.get(\"max_depth\")\n",
    "    min_samples_split = parameterization.get(\"min_samples_split\")\n",
    "    min_samples_leaf = parameterization.get(\"min_samples_leaf\")\n",
    "    \n",
    "    # Train the model with given hyperparameters\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and calculate MSE on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "    # Return the MSE as the objective value to minimize\n",
    "    return {\"objective_value\": mse}\n",
    "\n",
    "\n",
    "\n",
    "# Define the parameters to optimize\n",
    "parameters = [\n",
    "    {\"name\": \"n_estimators\", \"type\": \"range\", \"bounds\": [10, 200], \"value_type\": \"int\"},\n",
    "    {\"name\": \"max_depth\", \"type\": \"range\", \"bounds\": [5, 50], \"value_type\": \"int\"},\n",
    "    {\"name\": \"min_samples_split\", \"type\": \"range\", \"bounds\": [2, 20], \"value_type\": \"int\"},\n",
    "    {\"name\": \"min_samples_leaf\", \"type\": \"range\", \"bounds\": [1, 10], \"value_type\": \"int\"}\n",
    "]\n",
    "\n",
    "# Run the optimization\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=parameters,\n",
    "    evaluation_function=evaluation_function,\n",
    "    objective_name='objective_value',\n",
    "    minimize=True,\n",
    "    total_trials=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f6b6d5",
   "metadata": {},
   "source": [
    "Now lets check our parameters and objective value. As you can see, the MSE score went down by a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921febb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters:\", best_parameters)\n",
    "print(\"Original Score:\", mse)\n",
    "print(\"Hyperparameter Tuned Score\", values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f86161",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we used Bayesian Optimization with the Ax library to efficiently find the minimum of a synthetic function, optimize within a dataset, and perform hyperparameter tuning on another model. Ax facilitated the process of defining the experiment, running the optimization, and analyzing the results. \n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Ax Documentation](https://ax.dev/)\n",
    "- [More on Bayesian Optimization](https://arxiv.org/abs/1807.02811)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
