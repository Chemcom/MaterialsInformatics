{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sp8rks/MaterialsInformatics/blob/main/worked_examples/random_forest/RF%20example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Example\n",
    "For this notebook I'll be pulling some data from Materials Project. I'll use the old api using my MyPymatgen virtual environment\n",
    "\n",
    "## Video \n",
    "\n",
    "https://www.youtube.com/watch?v=X6BXE3Bln5M&list=PLL0SWcFqypCl4lrzk1dMWwTUrzQZFt7y0&index=20 (Ensemble techniques)\n",
    "\n",
    "Let's start by getting our API key loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install pymatgen\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/My Drive/teaching/5540-6640 Materials Informatics/MaterialsInformatics/worked_examples/random_forest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "!pip install pymatgen\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "%cd /content/drive/My Drive/teaching/5540-6640 Materials Informatics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab our API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taylo\\AppData\\Local\\Temp\\ipykernel_11424\\695778577.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "c:\\Users\\taylo\\miniconda3\\envs\\MatInformatics2\\Lib\\site-packages\\pymatgen\\ext\\matproj_legacy.py:164: UserWarning: You are using the legacy MPRester. This version of the MPRester will no longer be updated. To access the latest data with the new MPRester, obtain a new API key from https://materialsproject.org/api and consult the docs at https://docs.materialsproject.org/ for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymatgen.ext.matproj import MPRester\n",
    "import os\n",
    "#if running locally\n",
    "#filename = r'G:\\My Drive\\teaching\\5540-6640 Materials Informatics\\old_apikey.txt'\n",
    "#if running google Colab\n",
    "filename = r'old_apikey.txt'\n",
    "\n",
    "\n",
    "def get_file_contents(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            # It's assumed our file contains a single line,\n",
    "            # with our API key\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(\"'%s' file not found\" % filename)\n",
    "\n",
    "\n",
    "Sparks_API = get_file_contents(filename)\n",
    "mpr = MPRester(Sparks_API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab some data to work with. We'll grab chlorides within 1 meV of the convex hull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=('pretty_formula', 'band_gap',\n",
    "                           \"density\", 'formation_energy_per_atom', 'volume'))\n",
    "\n",
    "# grab some props for stable chlorides\n",
    "criteria = {'e_above_hull': {'$lte': 0.001},'elements':{'$all':['Cl']}}\n",
    "# criteria2 = {'e_above_hull': {'$lte': 0.02},'elements':{'$all':['O']},\n",
    "#              'band_gap':{'$ne':0}}\n",
    "\n",
    "props = ['pretty_formula', 'band_gap', \"density\",\n",
    "         'formation_energy_per_atom', 'volume']\n",
    "entries = mpr.query(criteria=criteria, properties=props)\n",
    "\n",
    "i = 0\n",
    "for entry in entries:\n",
    "    df.loc[i] = [entry['pretty_formula'], entry['band_gap'], entry['density'],\n",
    "                 entry['formation_energy_per_atom'], entry['volume']]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "RNG_SEED = 42\n",
    "np.random.seed(seed=RNG_SEED)\n",
    "\n",
    "X = df[['band_gap','formation_energy_per_atom','volume']]\n",
    "y = df['density']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RNG_SEED)\n",
    "rf = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('the r2 score is',r2)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('the mean absolute error is',mae)\n",
    "rmse_val = mean_squared_error(y_test, y_pred)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model isn't too great alone, but what if we add CBFV features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CBFV import composition\n",
    "import time\n",
    "\n",
    "rename_dict = {'density': 'target', 'pretty_formula':'formula'}\n",
    "df = df.rename(columns=rename_dict)\n",
    "\n",
    "\n",
    "RNG_SEED = 42\n",
    "np.random.seed(seed=RNG_SEED)\n",
    "\n",
    "X = df[['formula','band_gap','formation_energy_per_atom','volume']]\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RNG_SEED)\n",
    "\n",
    "X_train, y_train, formulae_train, skipped_train = composition.generate_features(df, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "X_test, y_test, formulae_train, skipped_train = composition.generate_features(df, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "\n",
    "\n",
    "#technically we should scale and normalize our data here... but lets skip it for now\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate the training time\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "rf = RandomForestRegressor(max_depth=4, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('the r2 score is',r2)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('the mean absolute error is',mae)\n",
    "rmse_val = mean_squared_error(y_test, y_pred)\n",
    "print(\"Training time:\", training_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way better! Our R^2 went way up, and our MAE went way down.\n",
    "\n",
    "# Grid Search Hyperparameter Tuning\n",
    "\n",
    "Now let's try one more time, but this time we'll do hyperparameter tuning!\n",
    "Note- we're going to reduce our data down to just 300 points during hyperparameter tuning or it will take foreeeeeeevvvvveeerrr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "# Select a subset of the dataframe with 300 data points\n",
    "subset_df = df.sample(n=300, random_state=RNG_SEED)\n",
    "\n",
    "# Split the subset into training and testing sets\n",
    "rename_dict = {'density': 'target', 'pretty_formula':'formula'}\n",
    "subset_df = subset_df.rename(columns=rename_dict)\n",
    "RNG_SEED = 42\n",
    "np.random.seed(seed=RNG_SEED)\n",
    "X = subset_df[['formula','band_gap','formation_energy_per_atom','volume']]\n",
    "y = subset_df['target']\n",
    "\n",
    "#now do CBFV\n",
    "X_train, y_train, formulae_train, skipped_train = composition.generate_features(subset_df, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "X_test, y_test, formulae_train, skipped_train = composition.generate_features(subset_df, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Create the random forest regressor\n",
    "rf_subset = RandomForestRegressor(random_state=RNG_SEED)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search_subset = GridSearchCV(estimator=rf_subset, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Start the timer\n",
    "start_time_subset = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search_subset.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the training time\n",
    "training_time_subset = time.time() - start_time_subset\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_subset = grid_search_subset.best_params_\n",
    "best_score_subset = grid_search_subset.best_score_\n",
    "\n",
    "# Train the model with the best parameters\n",
    "rf_best_subset_grid = RandomForestRegressor(random_state=RNG_SEED, **best_params_subset)\n",
    "rf_best_subset_grid.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_subset = rf_best_subset_grid.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2_subset = r2_score(y_test, y_pred_subset)\n",
    "mae_subset = mean_absolute_error(y_test, y_pred_subset)\n",
    "rmse_subset = mean_squared_error(y_test, y_pred_subset)\n",
    "\n",
    "print(\"Best parameters (subset):\", best_params_subset)\n",
    "print(\"Best score (subset):\", best_score_subset)\n",
    "print(\"R2 score (subset):\", r2_subset)\n",
    "print(\"Mean absolute error (subset):\", mae_subset)\n",
    "print(\"Root mean squared error (subset):\", rmse_subset)\n",
    "print(\"Training time (subset):\", training_time_subset, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search hyperparameter tuning\n",
    "Now let's try random search hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Create the random forest regressor\n",
    "rf_subset = RandomForestRegressor(random_state=0)\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(estimator=rf_subset, param_distributions=param_grid, n_iter=10, cv=5, random_state=0)\n",
    "\n",
    "# Start the timer\n",
    "start_time_subset = time.time()\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search_subset.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the training time\n",
    "training_time_subset = time.time() - start_time_subset\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_subset = grid_search_subset.best_params_\n",
    "best_score_subset = grid_search_subset.best_score_\n",
    "\n",
    "# Train the model with the best parameters\n",
    "rf_best_subset_random = RandomForestRegressor(random_state=RNG_SEED, **best_params_subset)\n",
    "rf_best_subset_random.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_subset = rf_best_subset_random.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2_subset = r2_score(y_test, y_pred_subset)\n",
    "mae_subset = mean_absolute_error(y_test, y_pred_subset)\n",
    "rmse_subset = mean_squared_error(y_test, y_pred_subset)\n",
    "\n",
    "print(\"Best parameters (subset):\", best_params_subset)\n",
    "print(\"Best score (subset):\", best_score_subset)\n",
    "print(\"R2 score (subset):\", r2_subset)\n",
    "print(\"Mean absolute error (subset):\", mae_subset)\n",
    "print(\"Root mean squared error (subset):\", rmse_subset)\n",
    "print(\"Training time (subset):\", training_time_subset, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#featurize with all the data, not just the subset\n",
    "X_train, y_train, formulae_train, skipped_train = composition.generate_features(df, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "X_test, y_test, formulae_train, skipped_train = composition.generate_features(df, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model with the best parameters\n",
    "rf_best_subset_random.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the training time\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = rf_best_subset_random.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"R2 score:\", r2)\n",
    "print(\"Mean absolute error:\", mae)\n",
    "print(\"Root mean squared error:\", rmse)\n",
    "print(\"Training time:\", training_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree visualization\n",
    "Finally, we can do tree visualization with graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "# Pull out one tree from the forest\n",
    "tree = rf.estimators_[5]\n",
    "# Import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "# Pull out one tree from the forest\n",
    "tree = rf.estimators_[5]\n",
    "# Export the image to a dot file\n",
    "feature_list = list(X_train.columns)\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n",
    "# Use dot file to create a graph\n",
    "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "# Write graph to a png file\n",
    "graph.write_png('tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost \n",
    "\n",
    "#### Boosting vs Bagging \n",
    "\n",
    "XGBoost has emerged as one of the favorite and best classical models. \n",
    "- Random Forest uses a technique called “bagging,” which builds multiple decision trees independently and then averages their predictions to reduce variance and improve robustness\n",
    "\n",
    "- XGBoost, on the other hand, uses “boosting,” which builds trees sequentially, each new tree correcting the errors of the previous ones. This process reduces bias and improves accuracy by focusing on the more difficult parts of the data\n",
    "\n",
    "#### Gradient Boosting\n",
    "\n",
    "- Optimizes a loss function using a gradient descent. This minimizes errors more effectively than the random approach of Random Forest \n",
    "\n",
    "#### Overfitting\n",
    "\n",
    "- XGBoost has built in regularization which helps provide robustness against overfitting\n",
    "\n",
    "\n",
    "### Training\n",
    "\n",
    "Lets train the model on the same dataset as before (for comparison's sake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will import our libraries. We will be using the XGBoost library for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import time\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fit our model we need to clean our data to make sure that the features can be used in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean feature names by replacing invalid characters\n",
    "X_train.columns = [col.replace('[', '').replace(']', '').replace('<', '').replace('>', '') for col in X_train.columns]\n",
    "X_test.columns = [col.replace('[', '').replace(']', '').replace('<', '').replace('>', '') for col in X_test.columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will train our model. Some of the parameters you can change include the n_estimators (the number of trees the model will train), the learning_rate (the contribution of each tree to the model), and max_depth (how deep a tree goes which may increase overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize and train the XGBoost model\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, seed=42, learning_rate=0.3, max_depth=6)\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = xg_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets evaluate this model. We will calculate the R^2, Mean Average Error (MAE), and Root Mean Squared Error (RMSE) values to see how it did. Furthermore, we will put the feature importance scores into a dataframe and print the top 10 features. XGBoost is very useful for discovering what features contribute the most to making good predictions. This is good for when you need to pare down the number of features in your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"R2 score:\", r2)\n",
    "print(\"Mean absolute error:\", mae)\n",
    "print(\"Root mean squared error:\", rmse)\n",
    "print(\"Training time:\", training_time, \"seconds\")\n",
    "\n",
    "# Get feature importance scores from the XGBoost model\n",
    "importance_scores = xg_reg.get_booster().get_score(importance_type='weight')\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "importance_df = pd.DataFrame(importance_scores.items(), columns=['Feature', 'Importance'])\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "importance_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it yourself!\n",
    "\n",
    "- Find oxides with MPRester and load their formula and band gap energy\n",
    "- Create a CBFV with this data for use in training models \n",
    "- Create a RF model using this data and score it \n",
    "- Create a single decision tree model (using sklearn DecisionTreeClassifier) using this data and score it\n",
    "- How do the scores of the two models compare? Why? \n",
    "- Extract the feature importance from the RF model and see which features matter the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatInformatics2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
