{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c545d06",
   "metadata": {},
   "source": [
    "# Introduction to Bayesian Optimization\n",
    "\n",
    "In this notebook, we'll explore the basics of Bayesian Optimization, a powerful method for optimizing black-box functions. We'll implement a simple Bayesian Optimization algorithm from scratch using basic libraries like numpy and pandas.\n",
    "\n",
    "## Key Components\n",
    "- **Objective Function**: The function we want to optimize.\n",
    "- **Surrogate Model**: A model that approximates the objective function.\n",
    "- **Acquisition Function**: A function that guides the search for the optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88799a6f",
   "metadata": {},
   "source": [
    "# Setting Up the Environment\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "\n",
    "# Helper function to plot the objective function\n",
    "def plot_objective(x, y):\n",
    "    plt.plot(x, y, 'r:', label='Objective Function')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.title('Objective Function')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd514d",
   "metadata": {},
   "source": [
    "# Define the Objective Function\n",
    "\n",
    "We'll define a simple objective function to optimize. For demonstration purposes, we'll use the function \\( f(x) = -x^2 + 4 \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fedfc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the objective function\n",
    "def objective_function(x):\n",
    "    return -x**2 + 4\n",
    "\n",
    "# Generate data points for visualization\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = objective_function(x)\n",
    "\n",
    "# Plot the objective function\n",
    "plot_objective(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae38010",
   "metadata": {},
   "source": [
    "# Surrogate Model with Gaussian Process\n",
    "\n",
    "We'll use a Gaussian Process (GP) as our surrogate model. The GP will help us model the objective function and make predictions about its behavior.\n",
    "\n",
    "### Gaussian Kernel\n",
    "The Gaussian (RBF) kernel is a common choice for GP. It defines the covariance function, which measures the similarity between points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62891dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Gaussian Kernel\n",
    "def gaussian_kernel(x1, x2, length_scale=1.0, sigma_f=1.0):\n",
    "    sqdist = np.sum((x1 - x2)**2)\n",
    "    return sigma_f**2 * np.exp(-0.5 / length_scale**2 * sqdist)\n",
    "\n",
    "# Example usage of the Gaussian Kernel\n",
    "x1, x2 = np.array([1]), np.array([2])\n",
    "print(f\"Gaussian Kernel between {x1} and {x2}: {gaussian_kernel(x1, x2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23db3d66",
   "metadata": {},
   "source": [
    "# Acquisition Function\n",
    "\n",
    "The acquisition function helps us decide where to sample next. One popular choice is the Expected Improvement (EI) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54898d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Expected Improvement acquisition function\n",
    "def expected_improvement(x, x_sample, y_sample, gp, xi=0.01):\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu_sample_opt = np.max(y_sample)\n",
    "\n",
    "    with np.errstate(divide='warn'):\n",
    "        imp = mu - mu_sample_opt - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "\n",
    "    return ei\n",
    "\n",
    "# Example usage\n",
    "# To be filled during the live coding session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed295b46",
   "metadata": {},
   "source": [
    "# Bayesian Optimization Loop\n",
    "\n",
    "In this section, we'll implement the main loop of Bayesian Optimization, which includes updating the surrogate model and optimizing the acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed9eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Bayesian Optimization Loop\n",
    "def bayesian_optimization(n_iters, sample_loss, bounds, x0=None):\n",
    "    x_sample = np.array(x0)\n",
    "    y_sample = sample_loss(x_sample)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        # Update the Gaussian Process with new samples\n",
    "        gp.fit(x_sample, y_sample)\n",
    "        \n",
    "        # Propose the next sampling point by optimizing the acquisition function\n",
    "        x_next = propose_location(expected_improvement, x_sample, y_sample, gp, bounds)\n",
    "        \n",
    "        # Obtain the next sample from the objective function\n",
    "        y_next = sample_loss(x_next)\n",
    "        \n",
    "        # Append new sample to previous samples\n",
    "        x_sample = np.vstack((x_sample, x_next))\n",
    "        y_sample = np.vstack((y_sample, y_next))\n",
    "        \n",
    "    return x_sample, y_sample\n",
    "\n",
    "# Example usage\n",
    "# To be filled during the live coding session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f6942",
   "metadata": {},
   "source": [
    "# Visualization and Analysis\n",
    "\n",
    "Let's visualize the results of our optimization process, including the objective function, surrogate model, and acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e59f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to plot the Gaussian Process and samples\n",
    "def plot_gp(gp, x, y, x_sample, y_sample, x_next=None):\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    plt.plot(x, y, 'r:', label='Objective Function')\n",
    "    plt.plot(x, mu, 'b-', label='GP Mean')\n",
    "    plt.fill_between(x.ravel(), mu - 1.96*sigma, mu + 1.96*sigma, alpha=0.2, color='k')\n",
    "    plt.plot(x_sample, y_sample, 'r.', markersize=10, label='Samples')\n",
    "    if x_next is not None:\n",
    "        plt.axvline(x_next, color='k', linestyle='--', label='Next Sample')\n",
    "    plt.legend()\n",
    "    plt.title('Gaussian Process and Objective Function')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# To be filled during the live coding session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c06f264",
   "metadata": {},
   "source": [
    "# Real-World Application in Materials Informatics\n",
    "\n",
    "Bayesian Optimization can be used in materials informatics for tasks such as optimizing material properties. For instance, it can help in discovering materials with desired characteristics by efficiently exploring the space of material compositions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f51ef",
   "metadata": {},
   "source": [
    "# Q&A and Further Reading\n",
    "\n",
    "Feel free to ask any questions! For further reading, check out the following resources:\n",
    "- [Bayesian Optimization Blog](https://blog.alan.dev/bayesian-optimization)\n",
    "- [Gaussian Processes for Machine Learning](http://www.gaussianprocess.org/gpml/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86bb3e7",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we've built a simple Bayesian Optimization algorithm from scratch using basic libraries. We've covered the essential components, including the objective function, surrogate model, and acquisition function. Bayesian Optimization is a powerful tool for optimizing expensive and complex functions, and we encourage you to explore it further."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
