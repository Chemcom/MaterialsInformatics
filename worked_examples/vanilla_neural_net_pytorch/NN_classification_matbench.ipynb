{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sp8rks/MaterialsInformatics/blob/main/worked_examples/hyperparameter_opt/materials_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELFcx4WgrdXV"
   },
   "source": [
    "# Neural Network Classification\n",
    "\n",
    "In this notebook we will use a neural network to classify data sourced from MPRester. We will be using the Neural Network from PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrA6ZaIYhd7l"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "yXjOg8s6ehcm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from pymatgen.ext.matproj import MPRester\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/nanohub/lib/python3.12/site-packages/pymatgen/ext/matproj_legacy.py:168: UserWarning: You are using the legacy MPRester. This version of the MPRester will no longer be updated. To access the latest data with the new MPRester, obtain a new API key from https://materialsproject.org/api and consult the docs at https://docs.materialsproject.org/ for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up MPRester\n",
    "filename = r'G:\\My Drive\\teaching\\5540-6640 Materials Informatics\\old_apikey.txt'\n",
    "\n",
    "def get_file_contents(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(\"'%s' file not found\" % filename)\n",
    "\n",
    "Sparks_API = get_file_contents(filename)\n",
    "mpr = MPRester(Sparks_API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will be using is 3 element formulas containing Li, Na, or K paired with Oxygen. We will be using the band gap, density, formation energy per atom, volume, and density to try to predict the stability of each compound. After we collect the data we will standardize it to increase stability and consistency between points. Lastly, we will create a train test split for testing the model's accuracy after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2513/2513 [00:01<00:00, 1970.94it/s]\n"
     ]
    }
   ],
   "source": [
    "criteria = {\"band_gap\": {\"$gt\": 0}, 'nelements':3, 'elements':{\"$in\":[\"Li\", \"Na\", \"K\"], \"$all\": [\"O\"]}}\n",
    "props = ['band_gap', \"density\", 'formation_energy_per_atom', 'volume', 'density', 'e_above_hull']\n",
    "entries = mpr.query(criteria=criteria, properties=props)\n",
    "\n",
    "df = pd.DataFrame(entries)\n",
    "df['stable_structure'] = df['e_above_hull'].apply(lambda x: 1 if x < 0.1 else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "TargetVariable = 'stable_structure'\n",
    "Predictors = ['density', 'formation_energy_per_atom', 'volume', 'band_gap', 'density']\n",
    "\n",
    "X = df[Predictors].values\n",
    "y = df[TargetVariable].values\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Map target labels to a continuous range starting from 0\n",
    "unique_labels = pd.unique(y)\n",
    "label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}\n",
    "y = pd.Series(y).map(label_mapping).values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuEoRyF_iRsV"
   },
   "source": [
    "## Construct the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rq79CBx_iVnl"
   },
   "source": [
    "We will be using a PyTorch neural network. The batch size controls the number of samples that will be passed through the network at one time. Using batches speeds up and stabilizes training. The DataLoader creates mini-batches from the dataset. It's important to specify how many classes there are as that needs to match the number of output layers on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwxtaSddV8vJ",
    "outputId": "5a4832af-c902-4d12-a67a-eb50f9e31648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader for training and testing\n",
    "batch_size = 128\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Ensure the number of classes matches the range of target labels\n",
    "num_classes = len(torch.unique(y))\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a simple neural network using PyTorch’s nn.Sequential to stack layers. The network consists of an input layer, a hidden layer with ReLU activation, and an output layer. \n",
    "\n",
    "The input layer is the first point of the NN. Each \"neuron\" represents a single feature being inputted. \n",
    "\n",
    "The ReLU activation layer is a non-linear activation function commonly used in neural networks. The purpose of the activation function is to introduce non-linearity into the network, allowing it to learn more complex patterns in the data. \n",
    "\n",
    "The output layer produces the final predictions. In a classification task, the output layer typically uses a softmax function (applied internally by nn.CrossEntropyLoss in PyTorch) to convert the raw output into probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network using Sequential\n",
    "input_size = X_train.shape[1]  # Number of features\n",
    "hidden_size = 128\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, num_classes)\n",
    ")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the neural network for a number of epochs, updating the model parameters using backpropagation. Each epoch represents a full cycle through the entire training dataset. More epochs decreases the overall loss but runs the risk of increased overfitting. This creates models that aren't able to generalize to unseen data as well.\n",
    "\n",
    "The forward pass of the data makes predictions and the back passes updates the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.4236941337585449\n",
      "Epoch 2/50, Loss: 0.5728803873062134\n",
      "Epoch 3/50, Loss: 0.3380995988845825\n",
      "Epoch 4/50, Loss: 0.47411811351776123\n",
      "Epoch 5/50, Loss: 0.3759986460208893\n",
      "Epoch 6/50, Loss: 0.43363016843795776\n",
      "Epoch 7/50, Loss: 0.3244531750679016\n",
      "Epoch 8/50, Loss: 0.39847975969314575\n",
      "Epoch 9/50, Loss: 0.34177204966545105\n",
      "Epoch 10/50, Loss: 0.29518526792526245\n",
      "Epoch 11/50, Loss: 0.42364615201950073\n",
      "Epoch 12/50, Loss: 0.5403419733047485\n",
      "Epoch 13/50, Loss: 0.3400542736053467\n",
      "Epoch 14/50, Loss: 0.3173294961452484\n",
      "Epoch 15/50, Loss: 0.41944023966789246\n",
      "Epoch 16/50, Loss: 0.5429033637046814\n",
      "Epoch 17/50, Loss: 0.4024004638195038\n",
      "Epoch 18/50, Loss: 0.34837329387664795\n",
      "Epoch 19/50, Loss: 0.40579235553741455\n",
      "Epoch 20/50, Loss: 0.3782951235771179\n",
      "Epoch 21/50, Loss: 0.4241243600845337\n",
      "Epoch 22/50, Loss: 0.4534699618816376\n",
      "Epoch 23/50, Loss: 0.3941074013710022\n",
      "Epoch 24/50, Loss: 0.4809429943561554\n",
      "Epoch 25/50, Loss: 0.3781812787055969\n",
      "Epoch 26/50, Loss: 0.31916430592536926\n",
      "Epoch 27/50, Loss: 0.36678826808929443\n",
      "Epoch 28/50, Loss: 0.357563316822052\n",
      "Epoch 29/50, Loss: 0.3206513226032257\n",
      "Epoch 30/50, Loss: 0.35659700632095337\n",
      "Epoch 31/50, Loss: 0.4457997679710388\n",
      "Epoch 32/50, Loss: 0.3729311227798462\n",
      "Epoch 33/50, Loss: 0.4165090024471283\n",
      "Epoch 34/50, Loss: 0.4170408844947815\n",
      "Epoch 35/50, Loss: 0.49665534496307373\n",
      "Epoch 36/50, Loss: 0.5440906882286072\n",
      "Epoch 37/50, Loss: 0.4156017601490021\n",
      "Epoch 38/50, Loss: 0.4154345691204071\n",
      "Epoch 39/50, Loss: 0.4719245135784149\n",
      "Epoch 40/50, Loss: 0.35802868008613586\n",
      "Epoch 41/50, Loss: 0.3100571632385254\n",
      "Epoch 42/50, Loss: 0.3642300069332123\n",
      "Epoch 43/50, Loss: 0.421970397233963\n",
      "Epoch 44/50, Loss: 0.4104093909263611\n",
      "Epoch 45/50, Loss: 0.5039011836051941\n",
      "Epoch 46/50, Loss: 0.5450838208198547\n",
      "Epoch 47/50, Loss: 0.4858436584472656\n",
      "Epoch 48/50, Loss: 0.3189973533153534\n",
      "Epoch 49/50, Loss: 0.3332594633102417\n",
      "Epoch 50/50, Loss: 0.3650337755680084\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for features, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we will test the model on the test set to evaluate its performance. The accuracy is calculated as the number of correct predictions divided by the total number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.89662027833002%\n"
     ]
    }
   ],
   "source": [
    "# Testing loop\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Much like the Random Forest and Support Vector Machine models, Neural Networks can also undergo hyperparameter tuning to increase their performance. We will be using grid search and random search to find the best parameters of the NN. \n",
    "\n",
    "Grid search takes a matrix of specified parameters and tests every single combination. This can end up being very computationally expensive and slow depending on how large the search space is (and how long the model takes to train). Luckily this model is fast to train and so won't take that long to grid search. \n",
    "\n",
    "Random search tries randomly sampled parameters in a specified search space. This is different than grid search in that it won't always find the best possible combination. However, this method is a lot faster and less computationally expensive than grid search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "\n",
    "Set up the grid search space to test parameters from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter space for grid search\n",
    "hidden_sizes = [64, 128, 256]\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "num_epochs_list = [20, 50, 100]\n",
    "batch_sizes = [64, 128]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform hyperparameter tuning with grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 85.48707753479125\n",
      "Best parameters: {'hidden_size': 128, 'learning_rate': 0.01, 'num_epochs': 100, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "for hidden_size in hidden_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        for num_epochs in num_epochs_list:\n",
    "            for batch_size in batch_sizes:\n",
    "                # Create DataLoader for training and testing\n",
    "                train_dataset = TensorDataset(X_train, y_train)\n",
    "                test_dataset = TensorDataset(X_test, y_test)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                # Ensure the number of classes matches the range of target labels\n",
    "                num_classes = len(torch.unique(y))\n",
    "\n",
    "                # Define the neural network using Sequential\n",
    "                input_size = X_train.shape[1]  # Number of features\n",
    "\n",
    "                model = nn.Sequential(\n",
    "                    nn.Linear(input_size, hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_size, num_classes)\n",
    "                )\n",
    "\n",
    "                # Loss and optimizer\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                # Training loop\n",
    "                for epoch in range(num_epochs):\n",
    "                    for features, labels in train_loader:\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(features)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Testing loop\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    for features, labels in test_loader:\n",
    "                        outputs = model(features)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "\n",
    "                accuracy = 100 * correct / total\n",
    "\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_params = {\n",
    "                        'hidden_size': hidden_size,\n",
    "                        'learning_rate': learning_rate,\n",
    "                        'num_epochs': num_epochs,\n",
    "                        'batch_size': batch_size\n",
    "                    }\n",
    "\n",
    "print(\"Best accuracy:\", best_accuracy)\n",
    "print(\"Best parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "\n",
    "Set up the random sample search space. This is the same space as grid search but it will randomly sample points rather than trying every single combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter space\n",
    "hidden_sizes = [64, 128, 256]\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "num_epochs_list = [20, 50, 100]\n",
    "batch_sizes = [64, 128]\n",
    "\n",
    "# Number of random samples to try\n",
    "num_samples = 10\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the random search optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 85.08946322067594\n",
      "Best parameters: {'hidden_size': 128, 'learning_rate': 0.01, 'num_epochs': 50, 'batch_size': 128}\n"
     ]
    }
   ],
   "source": [
    "for _ in range(num_samples):\n",
    "    hidden_size = random.choice(hidden_sizes)\n",
    "    learning_rate = random.choice(learning_rates)\n",
    "    num_epochs = random.choice(num_epochs_list)\n",
    "    batch_size = random.choice(batch_sizes)\n",
    "\n",
    "    # Create DataLoader for training and testing\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Ensure the number of classes matches the range of target labels\n",
    "    num_classes = len(torch.unique(y))\n",
    "\n",
    "    # Define the neural network using Sequential\n",
    "    input_size = X_train.shape[1]  # Number of features\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, num_classes)\n",
    "    )\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Testing loop\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = {\n",
    "            'hidden_size': hidden_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'num_epochs': num_epochs,\n",
    "            'batch_size': batch_size\n",
    "        }\n",
    "\n",
    "print(\"Best accuracy:\", best_accuracy)\n",
    "print(\"Best parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it Yourself!\n",
    "\n",
    "- Using the MPRester API find 2 element oxides that are stable. Sample at least 5 different properties including band gap. Don't forget to clean the data!\n",
    "- If the band gap is between 0.5-3 then change the value to 1 to signifiy a semiconductor. If it's outside that range change the value to 0 to signify it's not a semiconductor (a metal or insulator).\n",
    "- Set up a NN to classify if something is a semiconductor or not. Make sure to create a train test split for validation!\n",
    "- Perform hyperparameter tuning on the model and compare the performance from pre-tuning to post-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "materials_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
